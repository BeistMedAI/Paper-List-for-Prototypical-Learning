<div id = "top"></div>

<div align="center">

[![](https://capsule-render.vercel.app/api?type=waving&height=200&color=0:0F172A,65:4F46E5,100:22D3EE&text=ğŸŒŸ%20Paper%20List%20for%20Prototypical%20Learning&fontSize=30&fontAlign=50&fontAlignY=40&fontColor=FFFFFF&desc=%E2%80%94%20Junhao%20Jia&descAlign=90&descAlignY=60&descSize=20)](#top)

</div>

<p align="center">
  <a href="https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning">
    <img alt="Stars" src="https://img.shields.io/github/stars/BeistMedAI/Paper-List-for-Prototypical-Learning?style=flat-square&logo=github&label=Stars&labelColor=0F172A&color=4F46E5" />
  </a>
  <a href="https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning/forks">
    <img alt="Forks" src="https://img.shields.io/github/forks/BeistMedAI/Paper-List-for-Prototypical-Learning?style=flat-square&logo=github&label=Forks&labelColor=0F172A&color=4F46E5" />
  </a>
  <a href="https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning/issues">
    <img alt="Issues" src="https://img.shields.io/github/issues/BeistMedAI/Paper-List-for-Prototypical-Learning?style=flat-square&label=Issues&labelColor=0F172A&color=22D3EE" />
  </a>
  <a href="https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning/blob/main/LICENSE">
    <img alt="Code License" src="https://img.shields.io/github/license/BeistMedAI/Paper-List-for-Prototypical-Learning?style=flat-square&label=Code%20License&labelColor=0F172A&color=22D3EE" />
  </a>
  <a href="https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning/blob/main/LICENSE-DOCS">
    <img alt="Docs License" src="https://img.shields.io/badge/Docs%20License-CC%20BY%204.0-22D3EE?style=flat-square&labelColor=0F172A" />
  </a>
  <br/>
  <img alt="Last update" src="https://img.shields.io/badge/Last%20update-2025.08-4F46E5?style=flat-square&labelColor=0F172A" />
  <img alt="PRs Welcome" src="https://img.shields.io/badge/PRs-Welcome-22D3EE?style=flat-square&labelColor=0F172A" />
  <a href="https://awesome.re"><img alt="Awesome" src="https://awesome.re/badge-flat.svg" /></a>
</p>


**ğŸ¦‰ Contributors: [Junhao Jia (23' HDU Undergraduate)](https://github.com/BeistMedAI), [Yifei Sun (22' HDU-ITMO Undergraduate)](https://diaoquesang.github.io/), [Yunyou Liu (23' HDU Undergraduate)](https://github.com/Yunyou-Liu), [Huangwei Chen (22' HDU Undergraduate)](https://huangwei-chen.github.io/), [Shuo Jiang (23' HDU Undergraduate)](https://github.com/JSLiam94), [Huangxing Lin (24' XDU Undergraduate)](https://github.com/Zoraaster1207), [Ziyan Luo (23' HNU Undergraduate)](https://github.com/Zoraaster1207), [Hanwen Zheng (23' HDU Undergraduate)](https://github.com/Zhenghanwen-zhw), [Yuting Shi (23' HDU Undergraduate)](https://github.com/sytttttttt)**

**ğŸ“ DeepWiki: [Generating GitHub Knowledge Base Documentation in One Click](https://deepwiki.com/BeistMedAI/Paper-List-for-Prototypical-Learning).**

**ğŸ“¦ Other resources: [1] [Paper-List-for-Medical-Anomaly-Detection](https://github.com/diaoquesang/Paper-List-for-Medical-Anomaly-Detection), [2] [Paper List for Medical Reasoning Large Language Models](https://github.com/HovChen/Paper-List-for-Medical-Reasoning-Large-Language-Models)**

### Welcome to join us by contacting: 23080631@hdu.edu.cn.

<div>
<p align="center">
  <a href="https://www.hdu.edu.cn/"><img src="logos/HDU.png" height="42" alt="HDU" /></a>&nbsp;&nbsp;
  <a href="https://www.zju.edu.cn/"><img src="logos/ZJU.png" height="42" alt="ZJU" /></a>&nbsp;&nbsp;
  <a href="https://www.xdu.edu.cn/"><img src="logos/XDU.png" height="42" alt="XDU" /></a>&nbsp;&nbsp;
  <a href="https://www.hnu.edu.cn/"><img src="logos/HNU.png" height="42" alt="HNU" /></a>&nbsp;&nbsp;
  <a href="https://en.itmo.ru/"><img src="logos/ITMO.jpg" height="42" alt="ITMO" /></a>&nbsp;&nbsp;
  <a href="https://en.cuhk.edu.hk/"><img src="logos/CUHK-SZ.png" height="42" alt="CUHK-SZ" /></a>&nbsp;&nbsp;
  <a href="https://www.sribd.cn/"><img src="logos/SRIBD.png" height="42" alt="SRIBD" /></a>
</p>

</div>
The list covers interpretable, prototype/part-based vision papers, organized by Aâ€“K taxonomy (multi-label). Medical Imaging (J) is further split into organ/modality/task subcategories.

### Aâ€“K ç±»åˆ«é€Ÿè§ˆ Â· Aâ€“K at a glance
- [A. å¥ åŸº/é€šç”¨ ProtoPNet ç³»ä¸è¿‘äº² Â· Foundations & general prototype-part models](#s1)
- [B. Transformer/ViT åŸå‹æ–¹æ³• Â· Prototype-based Transformers/ViTs](#s2)
- [C. åˆ†å±‚/æ ‘/è¶…æ›²ç©ºé—´ Â· Hierarchies, trees, hyperbolic spaces](#s3)
- [D. åŸå‹è¡¨ç¤º/åŒ¹é…èŒƒå¼çš„æ‰©å±• Â· Extension of Prototypical Representation/Matching Paradigm](#s4)  
  â†³ [D1. å¯å½¢å˜/å¯é€†/åƒç´ å¯¹é½](#s41) Â· [D2. éå‚æ•°/æ ¸/åˆ†å¸ƒå¼åŸå‹](#s42) Â· [D3. åŸå‹å…±äº«/åˆ†é…/è§’è‰²](#s43)
- [E. è¯­ä¹‰/æ¦‚å¿µå¯¹é½ï¼ˆå«å¤šæ¨¡æ€ï¼‰ Â· Semantic/concept alignment (incl. multimodal)](#s5)
- [F. è®­ç»ƒç­–ç•¥/åéªŒæ„å»º/å¯ç¼–è¾‘ Â· Training strategies, post-hoc, editable models](#s6)
- [G. è¯„æµ‹/åŸºå‡†/æ‰¹åˆ¤ Â· Evaluation, benchmarks, critiques](#s7)
- [H. ç»¼è¿° Â· Surveys](#s8)
- [I. ä»»åŠ¡ç»´åº¦ Â· Tasks (segmentation, keypoint, regression, UDA, MIL, multi-label)](#s9)
- [J. åŒ»å­¦å½±åƒ Â· Medical imaging](#s10)  
  â†³ [J1. Alzheimerâ€™s (MRI)](#s101) Â· [J2. Tumor (MRI/mpMRI)](#s102) Â· [J3. Breastâ€”Digital Mammography](#s103) Â· [J4. Chestâ€”CXR/COVID-19](#s104)  
  â†³ [J5. Cephalometric/Dental X-ray](#s105) Â· [J6. Pathologyâ€”WSI/MIL](#s106) Â· [J7. Retinaâ€”Fundus](#s107) Â· [J8. Dermatologyâ€”Skin](#s108) Â· [J9. General/Multi-organ](#s109)
- [K. ç²’åº¦å¼ºåŒ– Â· Patch/Part/Pixel](#s11)

## ğŸ” Quick Start 
- ğŸ§­ **Foundations**: This Looks Like That (NeurIPS 2019) â†’ ProtoTree (CVPR 2021) â†’ TesNet (ICCV 2021)
- ğŸ§© **Patch/Pixel Evidence**: PIP-Net (CVPR 2023) â†’ PixPNet (WACV 2024)
- ğŸ§  **Transformer-based Prototypes**: ViT-NeT (ICML 2022) â†’ ProtoPFormer (IJCAI 2024) â†’ ProtoViT (NeurIPS 2024)
- ğŸŒ² **Hierarchies / Hyperbolic**: Hierarchical Prototypes (AAAI 2019) â†’ HAPPI (ICCVW 2025)
- ğŸ¥ **Medical Imaging Track**: XProtoNet (CVPR 2021) â†’ PAMIL (MICCAI 2024) â†’ CIPL (TMI 2025)

> One-week onboarding? Read 1 paper + run 1 repo from each line above.

## ğŸ§° How to Use
- **By paradigm**: Need pixel-level evidence? See **K / D1**. Want a hierarchical reasoning chain? See **C**. Want ViT-coupled methods? Jump to **B**.
- **By task**: For segmentation / keypoint / MIL / UDA, see **I**. For medical imaging, use **J** and pick by organ/modality subcategory.
- **Code availability**: The :octocat: icon links to code. If itâ€™s missing, the work is theory-oriented or not yet released.
- **Timeline**: For the latest trends, start with **2024â€“2025** items in **B / D / K**.

## ğŸ“’ Cheatsheet
- **Prototype / Part**: Reusable local evidence units aligned to image patches.
- **Support / Oppose**: Some methods allow prototypes to contribute positive or negative evidence, enabling defensible conclusions.
- **Activation spillover**: Prototype activations bleed into irrelevant regions â†’ see pixel-aligned approaches such as PixPNet.
- **Hierarchical prototypes**: Multi-level explanations from parts â†’ concepts â†’ categories.

## âš ï¸ Pitfalls
- **Latent similarity â‰  semantic similarity**: Always validate with pixel/region visualizations (see **G** for evaluation papers).
- **Prototype redundancy**: Separating supportive vs. trivial prototypes (e.g., ST-ProtoPNet) improves readability and discrimination.

## âœï¸ Tips

- :octocat:: Code

- [ğŸ¥° : Star History](#s0)

## <div id = "s1"></div> A. Foundations & general prototype-part models 

- [[2019-NeurIPS]](https://proceedings.neurips.cc/paper/2019/file/adf7ee2dcf142b0e11888e72b43fcb75-Paper.pdf) **This looks like that: deep learning for interpretable image recognition** [:octocat:](https://github.com/jakesnell/prototypical-networks)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å¼€åˆ›â€œè¿™åƒé‚£â€çš„åŸå‹-éƒ¨ä»¶èŒƒå¼ï¼šå­¦ä¹ è‹¥å¹²åŸå‹éƒ¨ä»¶ï¼Œå¹¶ä»¥å›¾åƒå±€éƒ¨ä¸åŸå‹çš„åŒ¹é…ä½œä¸ºå†³ç­–è¯æ®ï¼Œç»™å‡ºå¯è§†åŒ–çš„é€æ­¥è§£é‡Šã€‚
  - Intro (EN): Foundational 'This Looks Like That' prototype-part paradigm: learns a set of prototypical parts and matches them to image patches to justify decisions.

<img width="1245" height="495" alt="image" src="https://github.com/user-attachments/assets/0d73a726-2e61-4ebd-9641-2d0a81ad4606" />

- [[2019-AAAI]](https://ojs.aaai.org/index.php/HCOMP/article/view/5265) **Interpretable Image Recognition with Hierarchical Prototypes** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨åŸå‹åŸºç¡€ä¸Šå¼•å…¥å±‚çº§ç»“æ„ï¼ˆä»éƒ¨ä»¶åˆ°æ¦‚å¿µï¼‰ï¼Œè®©è§£é‡Šå…·å¤‡â€œç”±ç»†åˆ°ç²—â€çš„è¯­ä¹‰å±‚æ¬¡ã€‚
  - Intro (EN): Introduces hierarchical prototypes (from parts to concepts), enabling explanations at multiple semantic levels.
 
<img width="1406" height="1016" alt="image" src="https://github.com/user-attachments/assets/10355364-1222-4078-8dd3-8ca1822d08d4" />

- [[2021-ECML PKDD]](https://arxiv.org/pdf/2011.02863) **This looks like that, because... explaining prototypes for interpretable image recognition** [:octocat:](https://github.com/M-Nauta/Explaining_Prototypes)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨åŸå‹è¯†åˆ«ï¼ˆå¦‚ ProtoPNetï¼‰åŸºç¡€ä¸Šï¼Œä¸ºæ¯ä¸ªåŸå‹è‡ªåŠ¨é‡åŒ–é¢œè‰²ï¼ˆè‰²ç›¸ï¼‰ã€å½¢çŠ¶ã€çº¹ç†ã€å¯¹æ¯”åº¦ä¸é¥±å’Œåº¦ç­‰å› ç´ çš„å½±å“ï¼Œç”Ÿæˆå¯è§†åŒ–+æ•°å€¼çš„å…¨å±€ä¸å±€éƒ¨è§£é‡Šï¼Œæ¾„æ¸…â€œè¿™åƒé‚£â€çš„ç›¸ä¼¼æ€§å«ä¹‰ï¼Œå¹¶è¯†åˆ«è¯¯å¯¼æ€§ä¸å†—ä½™åŸå‹ã€‚
  - Intro (EN): Builds on prototype-based recognition (e.g., ProtoPNet) by enriching visual prototypes with quantitative explanationsâ€”estimating how hue, shape, texture, contrast, and saturation influence each prototypeâ€”to produce global and local justifications that clarify â€œthis looks like that,â€ and surface misleading or redundant prototypes.
 
<img width="1377" height="353" alt="image" src="https://github.com/user-attachments/assets/1593c4f6-1f33-44de-befb-712bed72cc86" />

- [[2021-ICCV]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Interpretable_Image_Recognition_by_Constructing_Transparent_Embedding_Space_ICCV_2021_paper.pdf) **Interpretable image recognition by constructing transparent embedding space** [:octocat:](https://github.com/JackeyWang96/TesNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æ„å»ºâ€œé€æ˜åµŒå…¥ç©ºé—´â€ï¼Œä½¿åŸå‹ä¸ç‰¹å¾ç»´åº¦æ›´æ˜“äºäººè§£é‡Šï¼Œå‡å°‘é»‘ç›’æ„Ÿã€‚
  - Intro (EN): Builds a transparent embedding space to align features and prototypes with human-interpretable axes.
 
<img width="1290" height="506" alt="image" src="https://github.com/user-attachments/assets/3f71aab0-c6c9-4c5d-a8cf-afdede38cd53" />

- [[2021-ICMLW]](https://arxiv.org/pdf/2105.02968) **This looks like that... does it? shortcomings of latent space prototype interpretability in deep networks** [[other source]]([https://icml.cc/media/icml-2024/Slides/34227.pdf](https://icml.cc/virtual/2021/11382))
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŒ‡å‡ºæ½œåœ¨ç©ºé—´ä¸­çš„â€œç›¸ä¼¼æ€§â€æœªå¿…ç­‰ä»·äºè¾“å…¥ç©ºé—´çš„è¯­ä¹‰ç›¸ä¼¼ï¼Œç³»ç»Ÿæ€§åˆ†æåŸå‹è§£é‡Šçš„é™·é˜±ã€‚
  - Intro (EN): Analyzes pitfalls: similarity in latent space may not reflect semantic similarity in input space; cautions for prototype interpretability.
 
<img width="2175" height="517" alt="image" src="https://github.com/user-attachments/assets/c1be777e-c8d2-4d00-9855-87704a0d4f70" />

- [[2022-ECCV]](https://arxiv.org/pdf/2112.02902) **Interpretable image classification with differentiable prototypes assignment** [:octocat:](https://github.com/gmum/ProtoPool)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æ„å»ºäº† ProtoPool æ¨¡å‹ï¼Œå¼•å…¥å¯å…±äº«åŸå‹åº“ã€å¯åŒºåˆ† (differentiable) çš„åŸå‹ä¸ç±»åˆ«åˆ†é…æœºåˆ¶ï¼Œå¹¶æå‡ºèšç„¦ç›¸ä¼¼æ€§ï¼ˆfocal similarityï¼‰å‡½æ•°ä»¥å‡¸æ˜¾æœ€å…·ä»£è¡¨æ€§çš„è§†è§‰éƒ¨åˆ†ï¼Œå®ç°ç«¯åˆ°ç«¯æ­£å‘æ¨ç†ã€æ˜¾è‘—å‡å°‘åŸå‹æ•°é‡ã€æå‡å¯è§£é‡Šæ€§ä¸è®­ç»ƒæ•ˆç‡ã€‚è¯¥æ¨¡å‹åœ¨ CUBâ€‘200â€‘2011 å’Œ Stanford Cars æ•°æ®é›†ä¸Šä»¥æ˜æ˜¾æ›´å°‘çš„åŸå‹å–å¾—äº†ç«äº‰æ€§ç”šè‡³ä¼˜è¶Šçš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå¹¶é€šè¿‡ç†è®ºåˆ†æä¸ç”¨æˆ·ç ”ç©¶éªŒè¯åŸå‹æ›´å…·è¾¨è¯†æ€§ã€‚ 
  - Intro (EN): Presents ProtoPool, an interpretable prototypeâ€‘based image classification model that introduces a differentiable assignment of prototypes to classes, reuse of prototypes across classes, and a focal similarity function to emphasize salient features. It enables endâ€‘toâ€‘end positive reasoning, significantly reduces the number of prototypes, simplifies training (avoiding pruning stages), and achieves stateâ€‘ofâ€‘theâ€‘art or competitive accuracy on CUBâ€‘200â€‘2011 and Stanford Cars datasets. Theoretical analysis and user studies demonstrate that its prototypes are more distinctive and interpretable.
 
<img width="1234" height="619" alt="image" src="https://github.com/user-attachments/assets/ed18ef47-b3b1-4910-b36b-fc6f56e8edd1" />

- [[2022-ICPR]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956087&casa_token=mV6CJ9r3J_8AAAAA:11F0UeuihJi9QxN60t18atyV_XRbByfQ35jW-jqCF-8IGwsb83VsWKv7ZWamDQncH_zB-2My1w&tag=1) **Multi-grained interpre table network for image recognition** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºäº†ä¸€ç§â€œå¤šç²’åº¦å¯è§£é‡Šç½‘ç»œâ€ï¼ˆMultiâ€‘Grained Interpretable Network for Image Recognitionï¼‰ï¼Œé€šè¿‡å°†ç±»ç»„ç»‡æˆå±‚çº§ç»“æ„å¹¶åˆ†é…å¤šä¸ªç²’åº¦çš„æ ‡ç­¾ï¼Œä½¿ç”¨æ ‘çŠ¶ç»“æ„çš„åˆ†ç±»å™¨åœ¨ç²—ç²’åº¦åˆ°ç»†ç²’åº¦çš„å±‚æ¬¡ä¸Šé€æ­¥è¿›è¡Œåˆ†ç±»å†³ç­–ï¼Œå¹¶å¯ä¸ºæ¯ä¸€æ­¥æä¾›å¯è§£é‡Šçš„å†³ç­–è·¯å¾„ã€‚åœ¨ CUBâ€‘200â€‘2011 ä¸ Stanford Cars ç­‰ç»†ç²’åº¦å›¾åƒè¯†åˆ«æ•°æ®é›†ä¸Šå®ç°äº†å…·æœ‰ç«äº‰åŠ›çš„åˆ†ç±»æ€§èƒ½ï¼ŒåŒæ—¶å¯¹æŠ— FGSM å’Œ PGD ç­‰å¯¹æŠ—æ ·æœ¬æ˜¾ç¤ºå‡ºæ›´é«˜çš„é²æ£’æ€§ã€‚
  - Intro (EN): Proposes a Multiâ€‘Grained Interpretable Network for Image Recognition that organizes object classes into a hierarchical tree and assigns multiâ€‘grained labels to training images. A treeâ€‘structured classifier learns features at different granularity levels and makes predictions from coarse to fine, offering a clear decision pathway with explanations at each step. The method achieves competitive accuracy on fineâ€‘grained datasets like CUBâ€‘200â€‘2011 and Stanford Cars, and demonstrates improved robustness to adversarial attacks such as FGSM and PGD.
 
<img width="1540" height="761" alt="image" src="https://github.com/user-attachments/assets/56ed3a48-6583-4b2d-aa1e-d627f954ca3c" />

- [[2022-MICCAI]](https://arxiv.org/pdf/2209.12420) **Knowledge distillation to ensemble global and interpretable prototype-based mammogram classification models** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨ä¹³è…ºXçº¿åˆ†ç±»ä¸­ç”¨è’¸é¦æ•´åˆå¯è§£é‡ŠåŸå‹ä¸å¼ºè¡¨å¾ï¼Œå…¼é¡¾æ€§èƒ½ä¸å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Uses knowledge distillation to combine interpretable prototypes with strong features for mammography classification.
 
<img width="1481" height="712" alt="image" src="https://github.com/user-attachments/assets/072a390d-9ace-435c-93e0-e2fc479ff024" />

- [[2022-NN]](https://www.sciencedirect.com/science/article/pii/S0893608022001125?casa_token=6wzHMFuphHYAAAAA:OWLadVlFDialP0WffRwHPUUYzpaDGI1mq-Pf0g7qsbj2GupKLPzQzQNOoKD187RbI-qme5pWXQ) **Think positive: An interpretable neural network for image recognition** [:octocat:](#)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»â€œæ­£å‘è¯æ®â€è§’åº¦æ„å»ºå¯è§£é‡Šæ¨¡å‹ï¼Œå¼ºè°ƒæ”¯æŒæ€§åŸå‹å¯¹å†³ç­–çš„è´¡çŒ®ã€‚
  - Intro (EN): Focuses on positive evidence: emphasizes supporting prototypes as primary drivers of decisions.
 
<img width="1709" height="672" alt="image" src="https://github.com/user-attachments/assets/be0f430f-a835-4595-8b0e-a846ecf22229" />

- [[2023-ICCV]](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_Support_and_Trivial_Prototypes_for_Interpretable_Image_Classification_ICCV_2023_paper.pdf) **Learning Support and Trivial Prototypes for Interpretable Image Classification** [:octocat:](https://github.com/cwangrun/ST-ProtoPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŒºåˆ†â€˜æ”¯æŒæ€§åŸå‹â€™ä¸â€˜æ¬¡è¦/å†—ä½™åŸå‹â€™ï¼Œæå‡è§£é‡Šè´¨é‡ä¸åˆ¤åˆ«åŠ›ã€‚
  - Intro (EN): Separates supportive vs. trivial prototypes to enhance explanation quality and discrimination.
 
<img width="792" height="377" alt="image" src="https://github.com/user-attachments/assets/d7c5ba01-3f48-4a15-b078-b19c04876e44" />

- [[2023-MICCAI]](https://arxiv.org/pdf/2310.15741) **Interpretable medical image classification using prototype learning and privileged information** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å¼•å…¥ç‰¹æƒä¿¡æ¯ï¼ˆè®­ç»ƒæœŸå¯è§ã€æµ‹è¯•æœŸä¸å¯è§ï¼‰æå‡åŒ»å­¦åˆ†ç±»è§£é‡Šä¸æ€§èƒ½ã€‚
  - Intro (EN): Leverages privileged information (available only at training) to enhance medical classification and interpretability.
 
<img width="1207" height="805" alt="image" src="https://github.com/user-attachments/assets/8ce558b9-c52a-4969-9121-8fabb06732a1" />

- [[2023-TCSVT]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10250992&casa_token=vyLb4XK3tAEAAAAA:LjaFmlFfW3DSSuW-ySMLrWeudKccSFdANQpSfsMtSh9ST5qiPdtCAqsPET8YKsIobenQpL0DFQ) **Transparent embedding space for interpretable image recognition** [:octocat:](https://github.com/JackeyWang96/TesNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æ„å»ºâ€œé€æ˜åµŒå…¥ç©ºé—´â€ï¼Œä½¿åŸå‹ä¸ç‰¹å¾ç»´åº¦æ›´æ˜“äºäººè§£é‡Šï¼Œå‡å°‘é»‘ç›’æ„Ÿã€‚
  - Intro (EN): Builds a transparent embedding space to align features and prototypes with human-interpretable axes.
 
<img width="1445" height="606" alt="image" src="https://github.com/user-attachments/assets/c342600f-3aed-4e47-81b0-1493761769bf" />

- [[2025-ICLR]](https://openreview.net/forum?id=4sDicVEy6M) **What Do You See in Commonï¼ŸLearning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits** [:octocat:](https://github.com/Imageomics/HComPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨åŸå‹åŸºç¡€ä¸Šå¼•å…¥å±‚çº§ç»“æ„ï¼ˆä»éƒ¨ä»¶åˆ°æ¦‚å¿µï¼‰ï¼Œè®©è§£é‡Šå…·å¤‡â€œç”±ç»†åˆ°ç²—â€çš„è¯­ä¹‰å±‚æ¬¡ã€‚
  - Intro (EN): Introduces hierarchical prototypes (from parts to concepts), enabling explanations at multiple semantic levels.
 
<img width="1789" height="864" alt="image" src="https://github.com/user-attachments/assets/ed747977-4fee-4e04-9554-0efd815b28e9" />

- [[2025-ICLR_WD]](https://openreview.net/forum?id=HXwrppoSPc) **COMiX: Compositional explanations using prototypes** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç”¨â€˜åŸå‹+ç»„åˆâ€™ç”Ÿæˆå¯ç»„åˆè§£é‡Šï¼Œæå‡å¤æ‚åœºæ™¯çš„å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Compositional explanations built from prototypes to handle complex scenes.
 
<img width="1263" height="702" alt="image" src="https://github.com/user-attachments/assets/d81eb193-1339-4ea6-8c77-602843166a32" />

- [[2025-PR]](https://www.sciencedirect.com/science/article/pii/S0031320324006526?casa_token=cwSd-BCRwpEAAAAA:pszMddU-YKWSOts3MzueNXcvt_DKqBOaBd2-3NWzLNZ2QRzq0cvc5mGKB16r7R_lFHGXGYVchg) **Characteristic discriminative prototype network with detailed interpretation for classification** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å­¦ä¹ æ›´å…·åˆ¤åˆ«æ€§çš„ç‰¹å¾-åŸå‹ï¼Œå¹¶æä¾›ç»†ç²’åº¦å†³ç­–åˆ†è§£ã€‚
  - Intro (EN): Learns more discriminative feature-prototype pairs with fine-grained decision breakdowns.
 
<img width="1229" height="523" alt="image" src="https://github.com/user-attachments/assets/1b47b3e5-60be-465d-9a00-e92488e151d6" />

## <div id = "s2"></div> B. Prototype-based Transformers/ViTs 

- [[2022-ICML]](https://proceedings.mlr.press/v162/kim22g/kim22g.pdf) **Vit-net: Interpretable vision transformers with neural tree decoder** [:octocat:](https://github.com/jumpsnack/ViT-NeT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠåŸå‹/æ ‘ç»“æ„èå…¥ViTï¼Œå½¢æˆâ€œTransformerç‰¹å¾+å¯è§£é‡Šæ ‘è§£ç â€çš„ç»„åˆã€‚
  - Intro (EN): Fuses ViT features with a tree decoder, marrying transformer representations with transparent prototype reasoning.

<img width="1543" height="599" alt="image" src="https://github.com/user-attachments/assets/a5233cd8-7d40-49df-b618-d4a15fffcb10" />
  
- [[2024-IJCAI]](https://www.ijcai.org/proceedings/2024/168) **ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognitio** [:octocat:](https://github.com/zju-vipa/ProtoPFormer)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨ViTä¸Šä¸“æ³¨äºåŸå‹åŒ–çš„å±€éƒ¨éƒ¨ä»¶ï¼ŒæŠ‘åˆ¶èƒŒæ™¯å¹²æ‰°ï¼Œæå‡è§£é‡Šèšç„¦åº¦ã€‚
  - Intro (EN): Prototype-focused ViT that suppresses background distraction, sharpening attention on meaningful parts.
 
<img width="1200" height="480" alt="image" src="https://github.com/user-attachments/assets/8246c5d1-feb1-460b-8ec7-69961b19d9f3" />

- [[2024-ISBI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10635182&casa_token=kEs0mPH5s5QAAAAA:lb-VTTALa7DeUJ8OXO3ZaivR2Wl-kAcNgxdB6HiQgmf0ggkDUnYTFEDyj5Nv8fL2F4ZRiURW4g) **Explainable transformer prototypes for medical diagnoses** [:octocat:](https://github.com/NUBagcilab/r2r_proto)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŸºäºTransformerçš„åŒ»å­¦è¯Šæ–­åŸå‹ï¼Œç»“åˆæ³¨æ„åŠ›ä¸åŸå‹æ¿€æ´»ç»™å‡ºè§£é‡Šã€‚
  - Intro (EN): Transformer-based medical diagnosis with prototype activations as clear visual rationales.
 
<img width="1850" height="598" alt="image" src="https://github.com/user-attachments/assets/7c55ddd1-458a-46ff-b367-32e969b00fa3" />

- [[2024-NeurIPS]](https://proceedings.neurips.cc/paper_files/paper/2024/file/48dfc849640344e2d58df0b5bb78c33b-Paper-Conference.pdf) **Interpretable Image Classification with Adaptive Prototype-based Vision Transformers** [:octocat:](https://github.com/Henrymachiyu/ProtoViT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨ViTä¸­è‡ªé€‚åº”åœ°å­¦ä¹ éƒ¨ä»¶åŸå‹ï¼Œå¹¶ä¸æ³¨æ„åŠ›è”åŠ¨ä»¥æå‡å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Learns adaptive part prototypes within ViTs and couples them with attention for better interpretability.
 
<img width="1478" height="912" alt="image" src="https://github.com/user-attachments/assets/fb2f39da-ea5c-4ce4-8a08-bafe1dccfdc5" />

- [[2025-JBHI_UR]](https://arxiv.org/pdf/2506.10669) **PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis** [:octocat:](https://github.com/marziehoghbaie/PiPViT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨çœ¼åº•å›¾åƒä¸ŠæŠŠPatchåŸå‹ä¸ViTç»“åˆï¼Œå…¼é¡¾ç»†ç²’åº¦ç—…å˜ä¸å…¨å±€ç»“æ„ã€‚
  - Intro (EN): Combines patch prototypes and ViTs for retinal imaging to capture fine lesions and global structures.
 
<img width="2010" height="655" alt="image" src="https://github.com/user-attachments/assets/f04315b1-c792-4fc7-8968-be51d348aeee" />

- [[2025-ICASSP]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890753&casa_token=CK3WkOK9TVsAAAAA:pBren1C90XkdKu7gUBnv5ReeSpxZ0zmhf2FeYqh-2t0gPXfpQPYsMb9onOdSyABmObSQkB_7dQ) **Prototypical Part Transformer for Interpretable Image Recognition**
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠéƒ¨ä»¶åŸå‹ä¸Transformerç»“æ„ç»“åˆï¼Œç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ ä¸è§£é‡Šã€‚
  - Intro (EN): Integrates part prototypes into a Transformer, unifying representation and explanation.
 
<img width="1275" height="417" alt="image" src="https://github.com/user-attachments/assets/0178283a-d91c-49a4-8657-85b17888c718" />

## <div id = "s3"></div> C. Hierarchies, trees, hyperbolic spaces 

- [[2019-AAAI]](https://ojs.aaai.org/index.php/HCOMP/article/view/5265) **Interpretable Image Recognition with Hierarchical Prototypes**
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨åŸå‹åŸºç¡€ä¸Šå¼•å…¥å±‚çº§ç»“æ„ï¼ˆä»éƒ¨ä»¶åˆ°æ¦‚å¿µï¼‰ï¼Œè®©è§£é‡Šå…·å¤‡â€œç”±ç»†åˆ°ç²—â€çš„è¯­ä¹‰å±‚æ¬¡ã€‚
  - Intro (EN): Introduces hierarchical prototypes (from parts to concepts), enabling explanations at multiple semantic levels.
 
<img width="1406" height="1016" alt="image" src="https://github.com/user-attachments/assets/10355364-1222-4078-8dd3-8ca1822d08d4" />

- [[2021-CVPR]](https://openaccess.thecvf.com/content/CVPR2021/papers/Nauta_Neural_Prototype_Trees_for_Interpretable_Fine-Grained_Image_Recognition_CVPR_2021_paper.pdf) **Neural prototype trees for interpretable fine-grained image recognition** [:octocat:](https://github.com/M-Nauta/ProtoTree)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠåŸå‹åŒ¹é…ä¸å¯è§£é‡Šçš„æ ‘æ¨¡å‹ç»“åˆï¼Œæ¯ä¸ªèŠ‚ç‚¹å¯¹åº”â€œåƒ/ä¸åƒâ€çš„è¯æ®ï¼Œç”Ÿæˆå¯è¿½è¸ªçš„å†³ç­–è·¯å¾„ã€‚
  - Intro (EN): Combines prototype matching with decision trees; each node encodes supporting/opposing evidence for a transparent decision path.
 
<img width="1519" height="458" alt="image" src="https://github.com/user-attachments/assets/b3f1f407-970d-4b53-9ee0-e02632933fdb" />

- [[2022-ICML]](https://proceedings.mlr.press/v162/kim22g/kim22g.pdf) **Vit-net: Interpretable vision transformers with neural tree decoder** [:octocat:](https://github.com/jumpsnack/ViT-NeT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠåŸå‹/æ ‘ç»“æ„èå…¥ViTï¼Œå½¢æˆâ€œTransformerç‰¹å¾+å¯è§£é‡Šæ ‘è§£ç â€çš„ç»„åˆã€‚
  - Intro (EN): Fuses ViT features with a tree decoder, marrying transformer representations with transparent prototype reasoning.
 
<img width="1543" height="599" alt="image" src="https://github.com/user-attachments/assets/a5233cd8-7d40-49df-b618-d4a15fffcb10" />

- [[2023-JBHI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290723&casa_token=3ayrYdj6bekAAAAA:ND9zMo8PowTyi_hywubBzd7YzKCFIp4hlVR3PI6yDoVBye3j6SCoequKrnACUhRoPQp7H_cP3A) **Interpretable inference and classification of tissue types in histological colorectal cancer slides based on ensembles adaptive boosting prototype tree** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŸºäºé›†æˆ/Boostingçš„åŸå‹æ ‘ï¼Œåœ¨ç—…ç†åˆ‡ç‰‡ä¸Šè§£é‡Šç»„ç»‡ç±»å‹åˆ¤åˆ«ã€‚
  - Intro (EN): Ensemble/boosted prototype trees for tissue classification on histology slides with transparent rules.
 
<img width="1413" height="981" alt="image" src="https://github.com/user-attachments/assets/272bef36-75bc-4ce0-9484-cb8f5f9b9856" />

- [[2025-ICCVW_BEW]](https://openreview.net/forum?id=PcF9Q51rF1&noteId=KSZAX91CoY) **HAPPI: Hyperbolic Hierarchical Part Prototypes for Image Recognition** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨è¶…æ›²å‡ ä½•ä¸­å­¦ä¹ å±‚çº§åŸå‹ï¼Œæå‡å±‚æ¬¡ç»“æ„è¡¨è¾¾ä¸ç›¸ä¼¼åº¦åº¦é‡ã€‚
  - Intro (EN): Learns hierarchical prototypes in hyperbolic geometry to better encode tree-like relations.
 
<img width="1603" height="716" alt="image" src="https://github.com/user-attachments/assets/0c690ab4-4ffc-42a4-9d0d-024ea040a7f9" />

- [[2025-ICLR]](https://openreview.net/forum?id=4sDicVEy6M) **What Do You See in Commonï¼ŸLearning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits** [:octocat:](https://github.com/Imageomics/HComPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨åŸå‹åŸºç¡€ä¸Šå¼•å…¥å±‚çº§ç»“æ„ï¼ˆä»éƒ¨ä»¶åˆ°æ¦‚å¿µï¼‰ï¼Œè®©è§£é‡Šå…·å¤‡â€œç”±ç»†åˆ°ç²—â€çš„è¯­ä¹‰å±‚æ¬¡ã€‚
  - Intro (EN): Introduces hierarchical prototypes (from parts to concepts), enabling explanations at multiple semantic levels.
 
<img width="1518" height="735" alt="image" src="https://github.com/user-attachments/assets/e74f379c-8ebc-4ddb-8518-004f2ee27c34" />

- [[2025-JBHI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10955117&casa_token=sTYYDiTd42cAAAAA:JU_OQVG1R0u18yid6HvEDCOri1b_7N7f89WZinPRn83aVnceKKjvB2ksF4GGPi_JciMq0B08EQ) **Progressive mining and dynamic distillation of hierarchical prototypes for disease classification and localisation** [:octocat:](https://github.com/cwangrun/HierProtoPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æ¸è¿›æŒ–æ˜ä¸åŠ¨æ€è’¸é¦å±‚çº§åŸå‹ï¼ŒåŒæ—¶å…¼é¡¾å®šä½ä¸åˆ†ç±»ã€‚
  - Intro (EN): Progressive mining and dynamic distillation of hierarchical prototypes for joint localization/classification.
 
<img width="1667" height="431" alt="image" src="https://github.com/user-attachments/assets/548a094c-a241-46e2-b30a-85ef274613f4" />

## <div id = "s4"></div> D. Extension of Prototypical Representation/Matching Paradigm 

### <div id = "s41"></div> D1. Deformable, invertible, pixel-grounded 

- [[2022-CVPR]](https://openaccess.thecvf.com/content/CVPR2022/papers/Donnelly_Deformable_ProtoPNet_An_Interpretable_Image_Classifier_Using_Deformable_Prototypes_CVPR_2022_paper.pdf) **Deformable protopnet: An interpretable image classifier using deformable prototypes** [:octocat:](https://github.com/jdonnelly36/Deformable-ProtoPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å…è®¸åŸå‹å‘ç”Ÿå½¢å˜ä¸ä½ç§»ï¼Œä»¥é€‚åº”å½¢æ€å·®å¼‚å’Œå§¿æ€å˜åŒ–ï¼ŒåŒæ—¶ä¿æŒâ€˜éƒ¨ä»¶â€”ç›¸ä¼¼åº¦â€™çš„å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Allows prototypes to deform/translate to handle pose and morphology changes while preserving part-similarity explanations.
 
<img width="1258" height="295" alt="image" src="https://github.com/user-attachments/assets/79e4a8d7-7c97-41ac-ad10-2bfe52821dc3" />

- [[2023-CVPR]](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) **Pip-net: Patch-based intuitive prototypes for interpretable image classification** [:octocat:](https://github.com/M-Nauta/PIPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥Patchä¸ºç²’åº¦å­¦ä¹ ç›´è§‚åŸå‹ï¼Œçªå‡ºâ€œå±€éƒ¨è´´ç‰‡â€”æ¦‚å¿µâ€çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿äººå·¥æ ¡éªŒä¸çº é”™ã€‚
  - Intro (EN): Learns intuitive patch-level prototypes mapping local image patches to human-meaningful parts for easy auditing and correction.
 
<img width="1665" height="441" alt="image" src="https://github.com/user-attachments/assets/7f70f2fc-76c1-4ca1-9ab8-117b2acc0353" />

- [[2024-CEURW]](https://arxiv.org/pdf/2407.14277) **Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for Alzheimer's Disease classification** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘åŒ»å­¦å½±åƒåœºæ™¯ã€å¼ºè°ƒå±€éƒ¨Patch/åƒç´ æˆ–å¯é€†/å¯å½¢å˜æœºåˆ¶ï¼›é€šè¿‡å°†å›¾åƒå±€éƒ¨ä¸å­¦ä¹ åˆ°çš„åŸå‹åŒ¹é…ï¼Œç»™å‡ºâ€˜è¿™çœ‹èµ·æ¥åƒé‚£â€™çš„ç›´è§‚è¯æ®ã€‚
  - Intro (EN): Medical imaging focus; Emphasizes patches/pixels, or deformable/invertible mechanisms. Matches image regions to learned prototypes to provide intuitive 'this looks like that' evidence.
 
<img width="1342" height="871" alt="image" src="https://github.com/user-attachments/assets/deb95825-bb48-4afe-92fb-330a670d5bbd" />

- [[2024-ECCV]](https://arxiv.org/pdf/2407.12200) **This Probably Looks Exactly Like That: An Invertible Prototypical Network** [:octocat:](https://github.com/craymichael/ProtoFlow)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠåŸå‹ç½‘ç»œåšæˆå¯é€†ç»“æ„ï¼Œä¾¿äºä»åŸå‹é‡æ„è¾“å…¥ã€æ£€éªŒå¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Makes prototype networks invertible to reconstruct inputs from prototypes and audit explanations.
 
<img width="1433" height="523" alt="image" src="https://github.com/user-attachments/assets/64819d67-7ed3-46c5-9f98-43d1bd2f2aa8" />

- [[2024-WACV]](https://openaccess.thecvf.com/content/WACV2024/papers/Carmichael_Pixel-Grounded_Prototypical_Part_Networks_WACV_2024_paper.pdf) **Pixel-grounded prototypical part networks** [:octocat:](https://github.com/merlresearch/PixPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å°†åŸå‹æ˜¾å¼å¯¹é½åˆ°åƒç´ å±‚çº§ä¸å®ä¾‹è½®å»“ï¼Œç¼“è§£â€˜æ¿€æ´»å¤–æº¢â€™é—®é¢˜ã€‚
  - Intro (EN): Aligns prototypes to pixels and instance contours, mitigating activation spillover.
 
<img width="1840" height="525" alt="image" src="https://github.com/user-attachments/assets/3140a92b-334a-4ecd-a2d5-a4bf16c2424c" />

- [[2025-JBHI_UR]](https://arxiv.org/pdf/2506.10669) **PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis** [:octocat:](https://github.com/marziehoghbaie/PiPViT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨çœ¼åº•å›¾åƒä¸ŠæŠŠPatchåŸå‹ä¸ViTç»“åˆï¼Œå…¼é¡¾ç»†ç²’åº¦ç—…å˜ä¸å…¨å±€ç»“æ„ã€‚
  - Intro (EN): Combines patch prototypes and ViTs for retinal imaging to capture fine lesions and global structures.
 
<img width="2008" height="673" alt="image" src="https://github.com/user-attachments/assets/d97cd06c-8986-454b-b75e-61a001d1fd04" />

- [[2025-CVPRW]](https://openaccess.thecvf.com/content/CVPR2025W/TCV/papers/Singh_ProtoPatchNet_An_Interpretable_Patch-Based_Prototypical_Network_CVPRW_2025_paper.pdf) **ProtoPatchNet: An Interpretable Patch-Based Prototypical Network** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŸºäºPatchçš„åŸå‹ç½‘ç»œï¼Œå¼ºè°ƒå¯ç»„åˆçš„å±€éƒ¨è¯æ®ä¸ç®€æ´çš„å¯è§†åŒ–ã€‚
  - Intro (EN): Patch-based prototypical network with composable local evidence and concise visualization.

<img width="1388" height="532" alt="image" src="https://github.com/user-attachments/assets/5c7f7211-3d6d-444b-b11e-f6af5556e916" />

### <div id = "s42"></div> D2. Non-parametric, kernels, distributions 

- [[2023-ICLR]](https://openreview.net/forum?id=lh-HRYxuoRr) **This Looks Like It Rather Than That: ProtoKNN For Similarity-Based Classifiers** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥è¿‘é‚»/éå‚æ•°æ–¹å¼å®ç°â€œè¿™åƒå“ªä¸€äº›â€ï¼šå‡å°‘è®­ç»ƒæ—¶çš„å‚æ•°åŒ–å‡è®¾ï¼Œå¼ºè°ƒåŸºäºåº“çš„ç›¸ä¼¼å®ä¾‹è¯æ®ã€‚
  - Intro (EN): Non-parametric, nearest-neighbor flavored 'this looks like those' using stored exemplars instead of heavy parametrization.
 
<img width="1322" height="551" alt="image" src="https://github.com/user-attachments/assets/381fb53b-57b0-428b-9e7d-9eb7385d6bca" />

- [[2024-xAI]](https://arxiv.org/pdf/2404.14830) **CoProNN: Concept-Based Prototypical Nearest Neighbors for Explaining Vision Models** [:octocat:](https://github.com/TeodorChiaburu/beexplainable)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥æ¦‚å¿µä¸ºä¸­å¿ƒçš„åŸå‹è¿‘é‚»æ–¹æ³•ï¼šç”¨å¯å‘½åæ¦‚å¿µåŒ¹é…ç›¸ä¼¼æ ·æœ¬ã€‚
  - Intro (EN): Concept-centric prototypical nearest neighbors that match samples via nameable concepts.
 
<img width="1471" height="850" alt="image" src="https://github.com/user-attachments/assets/1ae5bb64-c607-486a-bb1e-f2a55eb6e7a9" />

- [[2025-AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/34233) **A Robust Prototype-Based Network with Interpretable RBF Classifier Foundations** [:octocat:](https://github.com/si-cim/cbc-aaai-2025)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥RBFä¸ºåŸºç¡€æ„å»ºé²æ£’åŸå‹åˆ†ç±»å™¨ï¼Œè¿æ¥æ ¸æ–¹æ³•ä¸å¯è§£é‡ŠåŸå‹ã€‚
  - Intro (EN): Grounds prototype classifiers in RBF theory, linking kernel methods and interpretable prototypes.
 
<img width="781" height="324" alt="image" src="https://github.com/user-attachments/assets/f5680728-62e2-49f1-80c8-457d08ce5cb8" />

- [[2025-CVPR]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Interpretable_Image_Classification_via_Non-parametric_Part_Prototype_Learning_CVPR_2025_paper.pdf) **Interpretable Image Classification via Non-parametric Part Prototype Learning** [:octocat:](https://github.com/zijizhu/proto-non-param)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å­¦ä¹ éå‚æ•°çš„éƒ¨ä»¶åŸå‹é›†åˆï¼Œå‡å°‘å¼ºå‡è®¾å¹¶æå‡å¯ç¼–è¾‘æ€§ã€‚
  - Intro (EN): Learns non-parametric sets of part prototypes, reducing assumptions and enhancing editability.
 
<img width="1389" height="776" alt="image" src="https://github.com/user-attachments/assets/5fdc93f5-d1dd-4b19-931c-db51a6ee8f39" />

- [[2025-TPAMI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10982376&casa_token=O7jGkn0rrFgAAAAA:BbH8z9dUJQb0WVxUm3QaDXeqq7IOLlkG7lJQsQkIK7prWfl568yeeKCL7mLXFj9X4JGMWUPG5Q&tag=1) **Mixture of gaussian-distributed prototypes with generative modelling for interpretable and trustworthy image recognition** [:octocat:](https://github.com/cwangrun/MGProto)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠåŸå‹å»ºæ¨¡ä¸ºé«˜æ–¯æ··åˆå¹¶ç»“åˆç”Ÿæˆå¼å»ºæ¨¡ï¼Œå¼ºåŒ–ä¸ç¡®å®šæ€§è¡¨è¾¾ä¸å¯ä¿¡åº¦ã€‚
  - Intro (EN): Models prototypes as Gaussian mixtures and couples with generative modeling for uncertainty-aware trustworthiness.
<img width="1192" height="476" alt="image" src="https://github.com/user-attachments/assets/e330ea47-66a5-44db-89de-bb962efd87fc" />

### <div id = "s43"></div> D3. Sharing, assignment, prototype roles 

- [[2021-SIGKDD]](https://dl.acm.org/doi/pdf/10.1145/3447548.3467245) **Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification** [:octocat:](https://github.com/gmum/ProtoPShare)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºâ€œåŸå‹éƒ¨ä»¶å…±äº«â€ï¼Œåœ¨ç±»åˆ«ä¹‹é—´å…±äº«æˆ–è¿ç§»å…¸å‹éƒ¨ä»¶ï¼Œæå‡æ ·æœ¬æ•ˆç‡ä¸å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Proposes prototypical part sharing across classes to improve sample efficiency and interpretability.
 
<img width="1202" height="457" alt="image" src="https://github.com/user-attachments/assets/af7694d4-5b2b-4cf1-b004-fde7600f93ef" />

- [[2022-ECCV]](https://arxiv.org/pdf/2112.02902) **Interpretable image classification with differentiable prototypes assignment** [:octocat:](https://github.com/gmum/ProtoPool)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å°†åŸå‹åˆ†é…å»ºæ¨¡ä¸ºå¯å¾®åˆ†è¿‡ç¨‹ï¼Œæå‡åŸå‹å­¦ä¹ çš„å¯æ§æ€§ä¸å¯è®­ç»ƒæ€§ã€‚
  - Intro (EN): Formulates prototype assignment as a differentiable process to better control learning and consistency.
 
<img width="1234" height="624" alt="image" src="https://github.com/user-attachments/assets/071babe7-f04e-4c80-b7e9-b93e4c167b37" />

- [[2023-CVPR]](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) **Pip-net: Patch-based intuitive prototypes for interpretable image classification** [:octocat:](https://github.com/M-Nauta/PIPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥Patchä¸ºç²’åº¦å­¦ä¹ ç›´è§‚åŸå‹ï¼Œçªå‡ºâ€œå±€éƒ¨è´´ç‰‡â€”æ¦‚å¿µâ€çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿äººå·¥æ ¡éªŒä¸çº é”™ã€‚
  - Intro (EN): Learns intuitive patch-level prototypes mapping local image patches to human-meaningful parts for easy auditing and correction.
 
<img width="1667" height="434" alt="image" src="https://github.com/user-attachments/assets/3e112df4-91a0-439e-b243-54112f3ba452" />

- [[2023-ECAI]](https://arxiv.org/pdf/2307.10404) **Interpreting and correcting medical image classification with pip-net** [:octocat:](https://github.com/M-Nauta/PIPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥Patchä¸ºç²’åº¦å­¦ä¹ ç›´è§‚åŸå‹ï¼Œçªå‡ºâ€œå±€éƒ¨è´´ç‰‡â€”æ¦‚å¿µâ€çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿äººå·¥æ ¡éªŒä¸çº é”™ã€‚
  - Intro (EN): Learns intuitive patch-level prototypes mapping local image patches to human-meaningful parts for easy auditing and correction.
 
<img width="1480" height="346" alt="image" src="https://github.com/user-attachments/assets/3a728c15-d479-412f-bad5-591b5527a4cb" />

- [[2023-ICCV]](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_Support_and_Trivial_Prototypes_for_Interpretable_Image_Classification_ICCV_2023_paper.pdf) **Learning Support and Trivial Prototypes for Interpretable Image Classification** [:octocat:](https://github.com/cwangrun/ST-ProtoPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŒºåˆ†â€˜æ”¯æŒæ€§åŸå‹â€™ä¸â€˜æ¬¡è¦/å†—ä½™åŸå‹â€™ï¼Œæå‡è§£é‡Šè´¨é‡ä¸åˆ¤åˆ«åŠ›ã€‚
  - Intro (EN): Separates supportive vs. trivial prototypes to enhance explanation quality and discrimination.
 
<img width="811" height="387" alt="image" src="https://github.com/user-attachments/assets/f7f647b6-d203-48b3-a51a-03e9fdd20376" />

- [[2024-MICCAI]](https://arxiv.org/pdf/2403.18328) **Pipnet3d: Interpretable detection of alzheimer in mri scans** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥Patchä¸ºç²’åº¦å­¦ä¹ ç›´è§‚åŸå‹ï¼Œçªå‡ºâ€œå±€éƒ¨è´´ç‰‡â€”æ¦‚å¿µâ€çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿äººå·¥æ ¡éªŒä¸çº é”™ã€‚
  - Intro (EN): Learns intuitive patch-level prototypes mapping local image patches to human-meaningful parts for easy auditing and correction.
<img width="1383" height="619" alt="image" src="https://github.com/user-attachments/assets/c460a5a1-5647-4fcc-85c6-eebb9da5db0c" />

## <div id = "s5"></div> E. Semantic/concept alignment (incl. multimodal) 

- [[2022-CVPR]](https://openaccess.thecvf.com/content/CVPR2022/papers/Keswani_Proto2Proto_Can_You_Recognize_the_Car_the_Way_I_Do_CVPR_2022_paper.pdf) **Proto2Proto: Can you recognize the car, the way I do?** [:octocat:](https://openaccess.thecvf.com/content/CVPR2022/papers/Keswani_Proto2Proto_Can_You_Recognize_the_Car_the_Way_I_Do_CVPR_2022_paper.pdf)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: è®©æ¨¡å‹å­¦ä¼šâ€œåƒäººä¸€æ ·â€è¯†åˆ«ï¼ˆä»¥æ±½è½¦ä¸ºä¾‹ï¼‰ï¼šå°†äººç±»æ„ŸçŸ¥çš„éƒ¨ä»¶/åŸå‹ä¸ç½‘ç»œåŸå‹å¯¹é½ã€‚
  - Intro (EN): Aligns human-perceived parts with learned prototypes (cars as example), making model evidence human-centric.
 
<img width="1720" height="629" alt="image" src="https://github.com/user-attachments/assets/59d27a7b-c69f-43f0-bf8e-250e407c6d16" />

- [[2023-ICLR]](https://openreview.net/forum?id=oiwXWPDTyNk) **Concept-level Debugging of Part-Prototype Networks** [:octocat:](https://github.com/abonte/protopdebug)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨â€˜æ¦‚å¿µå±‚â€™è°ƒè¯•åŸå‹ç½‘ç»œï¼šå‘ç°å¹¶ä¿®å¤è¯¯å¯¼æ€§æ¦‚å¿µï¼Œä»è€Œæ”¹å–„é¢„æµ‹ä¸è§£é‡Šã€‚
  - Intro (EN): Debugs part-prototype networks at the concept level, fixing misleading concepts to improve both accuracy and explanations.
 
<img width="1435" height="479" alt="image" src="https://github.com/user-attachments/assets/08d1e8ae-3981-424a-9866-701dbe107d24" />

- [[2023-NeurIPS]](https://openreview.net/forum?id=dCAk9VlegR) **This looks like those: Illuminating prototypical concepts using multiple visualizations** [:octocat:](https://github.com/Henrymachiyu/This-looks-like-those_ProtoConcepts)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç”¨å¤šç§å¯è§†åŒ–æ–¹å¼å‘ˆç°åŒä¸€æ¦‚å¿µåŸå‹ï¼Œå¸®åŠ©äººç±»ç†è§£æ¨¡å‹â€œçœ‹åˆ°â€çš„è¯­ä¹‰ã€‚
  - Intro (EN): Shows multiple visualizations for the same prototypical concept to reveal what the model 'sees'.
 
<img width="1160" height="639" alt="image" src="https://github.com/user-attachments/assets/1fc0f967-4323-43bf-b358-d26be45c94b0" />

- [[2023-WACV]](https://openaccess.thecvf.com/content/WACV2023/papers/Sacha_ProtoSeg_Interpretable_Semantic_Segmentation_With_Prototypical_Parts_WACV_2023_paper.pdf) **Protoseg: Interpretable semantic segmentation with prototypical parts** [:octocat:](https://github.com/gmum/proto-segmentation)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠâ€˜éƒ¨ä»¶åŸå‹â€™æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²ï¼Œåƒç´ çº§æ˜¾ç¤ºâ€œè¿™å—åŒºåŸŸåƒå“ªç§åŸå‹â€ã€‚
  - Intro (EN): Extends part-prototypes to semantic segmentation, grounding 'which region looks like which prototype' at pixel level.
 
<img width="1595" height="608" alt="image" src="https://github.com/user-attachments/assets/074b11c2-6ef4-40e5-8cb1-fff90f0225ad" />

- [[2024-AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/30109) **On the concept trustworthiness in concept bottleneck models** [:octocat:](https://github.com/hqhQAQ/ProtoCBM)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: è®¨è®ºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„æ¦‚å¿µå¯ä¿¡åº¦ï¼Œæé†’æ¦‚å¿µè¯¯æ ‡æˆ–æ¼‚ç§»çš„é£é™©ã€‚
  - Intro (EN): Studies concept bottleneck trustworthiness, warning against mislabeled or drifting concepts.
 
<img width="1528" height="669" alt="image" src="https://github.com/user-attachments/assets/994ee032-7500-4cb9-b723-b741ffe73aaa" />

- [[2024-CIKM]](https://dl.acm.org/doi/pdf/10.1145/3627673.3679795) **Semantic prototypes: Enhancing transparency without black boxes** [:octocat:](https://github.com/ails-lab/Semantic-Prototypes)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç”¨è¯­ä¹‰åŒ–åŸå‹æ›¿ä»£çº¯ç‰¹å¾åŸå‹ï¼Œå¢å¼ºäººå¯è¯»æ€§ä¸è·¨åŸŸæ³›åŒ–æ½œåŠ›ã€‚
  - Intro (EN): Replaces raw feature prototypes with semantic prototypes for readability and cross-domain generalization.
 
<img width="1204" height="400" alt="image" src="https://github.com/user-attachments/assets/ce7275cd-457a-4363-8d93-bd237827b95b" />

- [[2024-CVPR]](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_MCPNet_An_Interpretable_Classifier_via_Multi-Level_Concept_Prototypes_CVPR_2024_paper.pdf) **Mcpnet: An interpretable classifier via multi-level concept prototypes** [:octocat:](https://eddie221.github.io/MCPNet/)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºå¤šå±‚çº§æ¦‚å¿µåŸå‹ï¼ˆä»éƒ¨ä»¶åˆ°æ¦‚å¿µï¼‰ï¼Œåœ¨ä¿æŒå‡†ç¡®ç‡çš„åŒæ—¶æå‡è§£é‡Šçš„å±‚æ¬¡æ€§ã€‚
  - Intro (EN): Multi-level concept prototypes from parts to concepts, offering layered explanations without sacrificing accuracy.
 
<img width="1414" height="701" alt="image" src="https://github.com/user-attachments/assets/55df0d6c-5137-46bd-9955-75132da931c1" />

- [[2024-CVPRW]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678010&casa_token=lqhBOsX5d7MAAAAA:4QCVH9ep9WEyQt9OMjfxH_vqrxKOKLN1qkZNhiIk1-wQ5dm_omZ-9f7EUI_wKshDgeooaAacgg) **Understanding the (extra-) ordinary: Validating deep model decisions with prototypical concept-based explanations** [:octocat:](https://github.com/maxdreyer/pcx)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é€šè¿‡åŸå‹æ¦‚å¿µéªŒè¯æ¨¡å‹å†³ç­–æ˜¯å¦ç¬¦åˆå¸¸è¯†/é¢†åŸŸçŸ¥è¯†ï¼Œè¯†åˆ«â€˜å¹³å‡¡/éå¹³å‡¡â€™è¯æ®ã€‚
  - Intro (EN): Validates whether decisions rely on ordinary vs. extra-ordinary prototypical concepts, aligning with domain knowledge.
 
<img width="731" height="722" alt="image" src="https://github.com/user-attachments/assets/b1cbb7ba-9b03-4402-aea0-faf98da074f6" />

- [[2024-MM]](https://openreview.net/forum?id=60Uyf4UumM) **Align2Concept: Language Guided Interpretable Image Recognition by Visual Prototype and Textual Concept Alignment** [:octocat:](https://openreview.net/forum?id=60Uyf4UumM)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å¯¹é½è§†è§‰åŸå‹ä¸æ–‡æœ¬æ¦‚å¿µï¼Œä½¿è§£é‡Šä»â€˜åƒå“ªå—â€™ä¸Šå‡åˆ°â€˜æ˜¯ä»€ä¹ˆæ¦‚å¿µâ€™ã€‚
  - Intro (EN): Aligns visual prototypes with textual concepts, moving from 'where it looks like' to 'what concept it is'.
 
<img width="1508" height="611" alt="image" src="https://github.com/user-attachments/assets/dd620d7b-aeb6-436b-84db-0a287fba0019" />

- [[2024-NeurIPSW]](https://arxiv.org/pdf/2410.18705?) **Exploiting Interpretable Capabilities with Concept-Enhanced Diffusion and Prototype Networks** [:octocat:](https://github.com/acarballocastro/ConceptEnhanced)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠæ‰©æ•£æ¨¡å‹çš„æ¦‚å¿µèƒ½åŠ›ä¸åŸå‹ç½‘ç»œç»“åˆï¼Œæ¢ç´¢æ›´å¼ºçš„å¯è§£é‡Šç”Ÿæˆ+åˆ¤åˆ«ã€‚
  - Intro (EN): Combines diffusion-model concepts with prototype networks for stronger interpretable generation+discrimination.
 
<img width="1321" height="728" alt="image" src="https://github.com/user-attachments/assets/3d25fd40-4351-45b5-afeb-1c1d5a33964c" />

- [[2024-plos]](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0311879) **An inherently interpretable deep learning model for local explanations using visual concepts** [:octocat:](https://github.com/mirzaahsan1986/CA_SoftNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç”¨äººå¯ç†è§£çš„è§†è§‰æ¦‚å¿µæ„å»ºå±€éƒ¨è§£é‡Šï¼Œè¿æ¥æ¦‚å¿µä¸è¯æ®åŒºåŸŸã€‚
  - Intro (EN): Uses human-understandable visual concepts to build local explanations linked to evidence regions.
 
<img width="1522" height="1047" alt="image" src="https://github.com/user-attachments/assets/a1fe5cdc-706f-409b-9bd2-f8e92e9e7406" />

- [[2024-TIP]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684084&casa_token=o3YFdb-f9QMAAAAA:S3kJ7gaQ-9UCvVoVhArWgi_sNYy3UvdflQL8_wrNWh4Dx5CI9V_379RWNQKe5Nc_gNH_BwwoYg) **Learning transferable conceptual prototypes for interpretable unsupervised domain adaptation** [:octocat:](https://drive.google.com/file/d/1b1EHFghiF1ExD-Cn1HYg75VutfkXWp60/view?usp=sharing)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å­¦ä¹ å¯è¿ç§»/å¯è·¨åŸŸçš„æ¦‚å¿µåŸå‹ï¼Œç”¨äºæ— ç›‘ç£åŸŸè‡ªé€‚åº”ä¸è§£é‡Šã€‚
  - Intro (EN): Learns transferable conceptual prototypes for unsupervised domain adaptation with transparent reasoning.
 
<img width="1487" height="509" alt="image" src="https://github.com/user-attachments/assets/ffe2fe4a-7dd2-4010-81b7-b728e774b907" />

- [[2024-WACV]](https://openaccess.thecvf.com/content/WACV2024/papers/Wan_Interpretable_Object_Recognition_by_Semantic_Prototype_Analysis_WACV_2024_paper.pdf) **Interpretable object recognition by semantic prototype analysis** [:octocat:](https://github.com/WanQiyang/SPANet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥è¯­ä¹‰åŸå‹åˆ†æç‰©ä½“è¯†åˆ«ï¼ŒæŠŠç±»åˆ«ç‰¹å¾æ‹†è§£ä¸ºå¯å‘½åçš„éƒ¨ä»¶ã€‚
  - Intro (EN): Analyzes object recognition via semantic prototypes, decomposing categories into nameable parts.
 
<img width="1301" height="703" alt="image" src="https://github.com/user-attachments/assets/cc1210f9-b11f-49e5-8b83-3b63175cf3dd" />

- [[2024-xAI]](https://arxiv.org/pdf/2404.14830) **CoProNN: Concept-Based Prototypical Nearest Neighbors for Explaining Vision Models** [:octocat:](https://github.com/TeodorChiaburu/beexplainable)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥æ¦‚å¿µä¸ºä¸­å¿ƒçš„åŸå‹è¿‘é‚»æ–¹æ³•ï¼šç”¨å¯å‘½åæ¦‚å¿µåŒ¹é…ç›¸ä¼¼æ ·æœ¬ã€‚
  - Intro (EN): Concept-centric prototypical nearest neighbors that match samples via nameable concepts.
 
<img width="1463" height="862" alt="image" src="https://github.com/user-attachments/assets/b7d45420-8f2a-4202-9aae-ff9655fe39eb" />

- [[2025-AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/32173) **ProtoArgNet: Interpretable Image Classification with Super-Prototypes and Argumentation** [:octocat:](https://github.com/H-Ayoobi/ProtoArgNet_AAAI)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºâ€˜è¶…åŸå‹+è®ºè¯â€™æœºåˆ¶ï¼Œè®©ä¸åŒåŸå‹æä¾›æ”¯æŒ/åé©³è¯æ®ï¼Œç”Ÿæˆå¯è¾©æŠ¤çš„ç»“è®ºã€‚
  - Intro (EN): Introduces super-prototypes and argumentation: prototypes can support or refute a decision for defendable conclusions.
 
<img width="1461" height="465" alt="image" src="https://github.com/user-attachments/assets/40487c43-fe9e-44a6-a35a-56237cac08bb" />

- [[2025-TNNLS]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10966440&casa_token=GgRZS965NTIAAAAA:OpRKYiHPFe85mP-ZRcXEF7c_jN2g0d0h_ba4E4K1fj4jeD5KWluGnsYX5gFGPKMGm3YuB6RX2A) **TraNCE: Transformative Nonlinear Concept Explainer for CNNs** [:octocat:](https://github.com/daslimo/TrANCE)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºå¯å˜æ¢çš„éçº¿æ€§æ¦‚å¿µè§£é‡Šå™¨ï¼Œä¸CNNç»“åˆæé«˜æ¦‚å¿µå±‚å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): A nonlinear, transformable concept explainer for CNNs to improve concept-level interpretability.
<img width="1536" height="832" alt="image" src="https://github.com/user-attachments/assets/0297883a-777a-435d-86aa-3f8a869e9e0a" />

## <div id = "s6"></div> F. Training strategies, post-hoc, editable models 

- [[2024-arXiv]](https://arxiv.org/pdf/2406.14675?) **This looks better than that: Better interpretable models with protopnext** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç»Ÿä¸€å¤šä¸ªåŸå‹éƒ¨ä»¶æ¨¡å—çš„æ”¹è¿›ä¸è®­ç»ƒæµç¨‹ï¼Œä¾¿äºå¤ç°ä¸å…¬å¹³æ¯”è¾ƒã€‚
  - Intro (EN): Unifies improvements and training practices for prototype-part models to ease reproduction and fair comparison.
 
<img width="1109" height="772" alt="image" src="https://github.com/user-attachments/assets/be12cf8b-7ca4-4280-a285-c8a44677cf28" />

- [[2024-ICML]](https://openreview.net/pdf?id=MlzUD5CKvZ) **Improving Prototypical Visual Explanations with Reward Reweighing, Reselection, and Retraining** [:octocat:](https://github.com/aaron-jx-li/R3-ProtoPNet)[[other source]](https://icml.cc/media/icml-2024/Slides/34227.pdf)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é€šè¿‡é‡åŠ æƒ/é‡é€‰/å†è®­ç»ƒæå‡åŸå‹è§£é‡Šçš„å¯ç”¨æ€§ä¸ä¸€è‡´æ€§ã€‚
  - Intro (EN): Reweight, reselect, and retrain to improve the usability and stability of prototype explanations.
 
<img width="1872" height="901" alt="image" src="https://github.com/user-attachments/assets/4a23c458-dbc3-40c4-951c-8b5b710d8b25" />

- [[2024-ICML]](https://openreview.net/pdf?id=jhWSzTO0Jl) **Post-hoc Part-Prototype Networks** [[other source]](https://hkustsmartlab.github.io/2024/06/08/icml/)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨é»‘ç›’æ¨¡å‹ä¹‹ä¸ŠåéªŒæ„å»ºâ€˜éƒ¨ä»¶-åŸå‹â€™è§£é‡Šå±‚ï¼Œæ— éœ€é‡è®­ä¸»å¹²ã€‚
  - Intro (EN): Builds a post-hoc part-prototype layer on top of a black-box model without retraining the backbone.
 
<img width="1362" height="1028" alt="image" src="https://github.com/user-attachments/assets/e06572f9-c909-4817-a6f6-2b9f9bf647c0" />

- [[2025-CVPR]](https://openaccess.thecvf.com/content/CVPR2025/papers/Donnelly_Rashomon_Sets_for_Prototypical-Part_Networks_Editing_Interpretable_Models_in_Real-Time_CVPR_2025_paper.pdf) **Rashomon sets for prototypical-part networks: Editing interpretable models in real-time** [:octocat:](https://github.com/jdonnelly36/proto-rset)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åˆ»ç”»åŸå‹ç½‘ç»œçš„â€˜æ‹‰ç»è’™é›†åˆâ€™ï¼Œæ”¯æŒåœ¨ç­‰ç²¾åº¦è§£ç©ºé—´å†…è¿›è¡Œå¯æ§/å®æ—¶ç¼–è¾‘ã€‚
  - Intro (EN): Characterizes Rashomon sets for prototype networks, enabling controlled, real-time edits within equal-accuracy solutions.

<img width="779" height="439" alt="image" src="https://github.com/user-attachments/assets/03aca78c-3e4c-49f6-a2e4-e4b6c61e200b" />

## <div id = "s7"></div> G. Evaluation, benchmarks, critiques 

- [[2021-ICMLW]](https://arxiv.org/pdf/2105.02968) **This looks like that... does it? shortcomings of latent space prototype interpretability in deep networks** [[other source]]([https://icml.cc/media/icml-2024/Slides/34227.pdf](https://icml.cc/virtual/2021/11382))
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŒ‡å‡ºæ½œåœ¨ç©ºé—´ä¸­çš„â€œç›¸ä¼¼æ€§â€æœªå¿…ç­‰ä»·äºè¾“å…¥ç©ºé—´çš„è¯­ä¹‰ç›¸ä¼¼ï¼Œç³»ç»Ÿæ€§åˆ†æåŸå‹è§£é‡Šçš„é™·é˜±ã€‚
  - Intro (EN): Analyzes pitfalls: similarity in latent space may not reflect semantic similarity in input space; cautions for prototype interpretability.
 
<img width="2175" height="517" alt="image" src="https://github.com/user-attachments/assets/c1be777e-c8d2-4d00-9855-87704a0d4f70" />

- [[2022-ECCV]](https://arxiv.org/pdf/2112.03184) **HIVE: Evaluating the human interpretability of visual explanations** [:octocat:](https://github.com/princetonvisualai/HIVE)[[]other source](https://princetonvisualai.github.io/HIVE/)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»äººç±»ç ”ç©¶è§’åº¦è¯„æµ‹å¯è§£é‡Šæ€§æ–¹æ³•ï¼Œæä¾›æ›´è´´è¿‘äººç±»è®¤çŸ¥çš„è¯„ä¼°æ¡†æ¶ä¸æŒ‡æ ‡ã€‚
  - Intro (EN): Human-grounded evaluation suite for interpretability, bringing cognitive validity to metrics.
 
<img width="1287" height="664" alt="image" src="https://github.com/user-attachments/assets/ccfe9962-58a8-425e-9790-c12f97132050" />

- [[2023-ICCV]](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Evaluation_and_Improvement_of_Interpretability_for_Self-Explainable_Part-Prototype_Networks_ICCV_2023_paper.pdf) **Evaluation and improvement of interpretability for self-explainable part-prototype networks** [:octocat:](https://github.com/hqhQAQ/EvalProtoPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºåŸå‹æ¨¡å‹çš„é‡åŒ–è¯„ä¼°ä¸æ”¹è¿›æ‰‹æ®µï¼Œå…³æ³¨åŸå‹è´¨é‡ã€ç¨³å®šæ€§å’Œå¯è¯»æ€§ã€‚
  - Intro (EN): Quantitatively evaluates and improves interpretability of self-explainable part-prototype networks.
 
<img width="1705" height="696" alt="image" src="https://github.com/user-attachments/assets/ea5c6cae-5136-4f4b-92ed-560d99533591" />

- [[2023-xAI]](https://arxiv.org/pdf/2307.14517) **The co-12 recipe for evaluating interpretable part-prototype image classifiers** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç»™å‡ºè¯„æµ‹åŸå‹æ–¹æ³•çš„ä¸€å¥—â€˜é…æ–¹â€™ä¸åè®®ï¼Œå¼ºè°ƒå¯å¤ç°ã€å¯é‡åŒ–çš„å¯¹æ¯”ã€‚
  - Intro (EN): Provides a reproducible 'recipe' to evaluate part-prototype methods with fair, quantitative comparisons.
 
<img width="1114" height="755" alt="image" src="https://github.com/user-attachments/assets/5dcfb888-7026-451c-9949-245632f75c50" />

- [[2024-AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/30154) **Interpretability benchmark for evaluating spatial misalignment of prototypical parts explanations** [:octocat:](https://github.com/gmum/interpretability-benchmark)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºç©ºé—´é”™ä½è¯„æµ‹åŸºå‡†ï¼šåŸå‹æ¿€æ´»ä¸çœŸå®éƒ¨ä½ä¸å¯¹é½çš„é—®é¢˜è¢«é‡åŒ–å¹¶å¯æ¯”è¾ƒã€‚
  - Intro (EN): Benchmarks spatial misalignment where prototype activations fail to align with true evidence regions.
 
<img width="1506" height="755" alt="image" src="https://github.com/user-attachments/assets/ca707e67-fd7d-4267-8086-beb0f3336a3c" />

- [[2024-AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/30109) **On the concept trustworthiness in concept bottleneck models** [:octocat:](https://github.com/hqhQAQ/ProtoCBM)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: è®¨è®ºæ¦‚å¿µç“¶é¢ˆæ¨¡å‹çš„æ¦‚å¿µå¯ä¿¡åº¦ï¼Œæé†’æ¦‚å¿µè¯¯æ ‡æˆ–æ¼‚ç§»çš„é£é™©ã€‚
  - Intro (EN): Studies concept bottleneck trustworthiness, warning against mislabeled or drifting concepts.
 
<img width="1455" height="647" alt="image" src="https://github.com/user-attachments/assets/071b6dbf-4257-40a4-b471-b8e3bef366d0" />

- [[2024-CVPRW]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678010&casa_token=lqhBOsX5d7MAAAAA:4QCVH9ep9WEyQt9OMjfxH_vqrxKOKLN1qkZNhiIk1-wQ5dm_omZ-9f7EUI_wKshDgeooaAacgg) **Understanding the (extra-) ordinary: Validating deep model decisions with prototypical concept-based explanations** [:octocat:](https://github.com/maxdreyer/pcx)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é€šè¿‡åŸå‹æ¦‚å¿µéªŒè¯æ¨¡å‹å†³ç­–æ˜¯å¦ç¬¦åˆå¸¸è¯†/é¢†åŸŸçŸ¥è¯†ï¼Œè¯†åˆ«â€˜å¹³å‡¡/éå¹³å‡¡â€™è¯æ®ã€‚
  - Intro (EN): Validates whether decisions rely on ordinary vs. extra-ordinary prototypical concepts, aligning with domain knowledge.
 
<img width="1599" height="485" alt="image" src="https://github.com/user-attachments/assets/f3c4ca26-e7f2-4966-bc75-ecca9e619039" />

- [[2024-xAI]](https://arxiv.org/pdf/2404.09917?) **Evaluating the Explainability of Attributes and Prototypes for a Medical Classification Model** [:octocat:](https://github.com/XRad-Ulm/Proto-Caps)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: è¯„ä¼°åŒ»ç–—æ¨¡å‹ä¸­â€˜å±æ€§+åŸå‹â€™è§£é‡Šçš„å¯é æ€§ä¸ä¸´åºŠå¯ç”¨æ€§ã€‚
  - Intro (EN): Evaluates the reliability and clinical utility of attribute+prototype explanations in medical models.
 
<img width="1004" height="742" alt="image" src="https://github.com/user-attachments/assets/9c2b9ca5-5178-479a-bf18-913580895bd2" />

- [[2024-xAI]](https://arxiv.org/pdf/2408.11401) **Revisiting FunnyBirds evaluation framework for prototypical parts networks** [:octocat:](https://github.com/hamer101/FunnyBirds_PrototypesRevisited)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é‡å®¡FunnyBirdsæ¡†æ¶ï¼Œæç¤ºåŸå‹æ–¹æ³•çš„è¯„æµ‹é™·é˜±ä¸æ”¹è¿›æ–¹å‘ã€‚
  - Intro (EN): Revisits the FunnyBirds framework to highlight pitfalls and fixes in prototype evaluation.

<img width="924" height="299" alt="image" src="https://github.com/user-attachments/assets/c2a48b79-174b-42d2-82df-7927ac82ef7e" />

## <div id = "s8"></div> H. Surveys 

- [[2024-biomedinformatics]](https://www.mdpi.com/2673-7426/4/4/115) **Part-Prototype Models in Medical Imaging Applications and Current Challenges** [:octocat:](#)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŒ»å­¦å½±åƒåŸå‹æ–¹æ³•ç»¼è¿°ï¼Œç³»ç»Ÿæ¢³ç†åº”ç”¨ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ã€‚
  - Intro (EN): Survey of prototype-based methods in medical imaging: applications, challenges, and future directions.
 
<img width="1465" height="587" alt="image" src="https://github.com/user-attachments/assets/a658d102-231a-413e-8524-70077f43928c" />

- [[2024-xAI]](https://arxiv.org/pdf/2403.20260) **Prototype-based interpretable breast cancer prediction models: Analysis and challenges** [:octocat:](https://github.com/ShreyasiPathak/multiinstance-learning-mammography)[:octocat:](https://github.com/ShreyasiPathak/prototype-based-breast-cancer-prediction)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä¹³è…ºå½±åƒæ–¹å‘çš„åŸå‹æ–¹æ³•ç»¼è¿°ä¸æŒ‘æˆ˜æ¢³ç†ã€‚
  - Intro (EN): Review and challenges in breast imaging with prototype-based models.
<img width="830" height="738" alt="image" src="https://github.com/user-attachments/assets/fe6c947a-e930-4deb-996a-2c9d5884c3c5" />

## <div id = "s9"></div> I. Tasks (segmentation, keypoint, regression, UDA, MIL, multi-label) 

- [[2021-SIGKDD]](https://dl.acm.org/doi/pdf/10.1145/3447548.3467245) **Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification** [:octocat:](https://github.com/gmum/ProtoPShare)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºâ€œåŸå‹éƒ¨ä»¶å…±äº«â€ï¼Œåœ¨ç±»åˆ«ä¹‹é—´å…±äº«æˆ–è¿ç§»å…¸å‹éƒ¨ä»¶ï¼Œæå‡æ ·æœ¬æ•ˆç‡ä¸å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Proposes prototypical part sharing across classes to improve sample efficiency and interpretability.
 
<img width="1322" height="352" alt="image" src="https://github.com/user-attachments/assets/00adfde3-1b50-48e4-9a6b-60a402fce49f" />

- [[2022-MICCAI]](https://arxiv.org/pdf/2208.00457) **INSightR-Net: interpretable neural network for regression using similarity-based comparisons to prototypical examples** [:octocat:](https://github.com/lindehesse/INSightR-Net)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŸºäºç›¸ä¼¼åº¦æ¯”è¾ƒçš„å¯è§£é‡Šå›å½’ï¼šç”¨åŸå‹ä½œä¸ºå‚ç…§è¿›è¡Œè¿ç»­æ•°å€¼é¢„æµ‹ä¸è§£é‡Šã€‚
  - Intro (EN): Similarity-based interpretable regression using prototypes as reference points for continuous predictions.
 
<img width="1382" height="641" alt="image" src="https://github.com/user-attachments/assets/6611a8a1-da2d-4644-afee-3690cbea92a2" />

- [[2023-CVPR]](https://openaccess.thecvf.com/content/ICCV2023/papers/Rymarczyk_ICICLE_Interpretable_Class_Incremental_Continual_Learning_ICCV_2023_paper.pdf) **Icicle: Interpretable class incremental continual learning** [:octocat:](https://github.com/gmum/ICICLE)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘å¢é‡å­¦ä¹ çš„åŸå‹æ–¹æ³•ï¼šåœ¨æ–°ç±»åˆ°æ¥æ—¶ä¿æŒæ—§ç±»è§£é‡Šçš„ä¸€è‡´æ€§ä¸ç¨³å®šæ€§ã€‚
  - Intro (EN): Prototype-based continual learning that maintains explanation consistency as new classes arrive.
 
<img width="1380" height="849" alt="image" src="https://github.com/user-attachments/assets/27d7a7c1-4b7d-4b7f-a4be-8f4412983427" />

- [[2023-ICLR]](https://openreview.net/forum?id=lh-HRYxuoRr) **This Looks Like It Rather Than That: ProtoKNN For Similarity-Based Classifiers** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥è¿‘é‚»/éå‚æ•°æ–¹å¼å®ç°â€œè¿™åƒå“ªä¸€äº›â€ï¼šå‡å°‘è®­ç»ƒæ—¶çš„å‚æ•°åŒ–å‡è®¾ï¼Œå¼ºè°ƒåŸºäºåº“çš„ç›¸ä¼¼å®ä¾‹è¯æ®ã€‚
  - Intro (EN): Non-parametric, nearest-neighbor flavored 'this looks like those' using stored exemplars instead of heavy parametrization.
 
<img width="1302" height="578" alt="image" src="https://github.com/user-attachments/assets/45d2a3d4-e728-431a-a7e9-248d654ac4c3" />

- [[2023-WACV]](https://openaccess.thecvf.com/content/WACV2023/papers/Sacha_ProtoSeg_Interpretable_Semantic_Segmentation_With_Prototypical_Parts_WACV_2023_paper.pdf) **Protoseg: Interpretable semantic segmentation with prototypical parts** [:octocat:](https://github.com/gmum/proto-segmentation)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠâ€˜éƒ¨ä»¶åŸå‹â€™æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²ï¼Œåƒç´ çº§æ˜¾ç¤ºâ€œè¿™å—åŒºåŸŸåƒå“ªç§åŸå‹â€ã€‚
  - Intro (EN): Extends part-prototypes to semantic segmentation, grounding 'which region looks like which prototype' at pixel level.
 
<img width="1577" height="597" alt="image" src="https://github.com/user-attachments/assets/e00a15d9-d421-439c-91d5-b42797f40c26" />

- [[2024-MICCAI]](https://arxiv.org/pdf/2406.12577) **Cephalometric Landmark Detection across Ages with Prototypical Network** [:octocat:](https://github.com/ShanghaiTech-IMPACT/CeLDA)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘å¤´å½±æµ‹é‡çš„å…³é”®ç‚¹æ£€æµ‹ï¼Œå¼•å…¥åŸå‹å…ˆéªŒä»¥æå‡è·¨å¹´é¾„æ³›åŒ–ã€‚
  - Intro (EN): Keypoint localization for cephalometric analysis with prototype priors to generalize across ages.
 
<img width="1363" height="791" alt="image" src="https://github.com/user-attachments/assets/841a4c74-dd49-46cc-a2fb-767364af0328" />

- [[2024-MICCAI]](https://papers.miccai.org/miccai-2024/paper/1022_paper.pdf) **Pamil: Prototype attention-based multiple instance learning for whole slide image classification** [:octocat:](https://github.com/Jiashuai-Liu/PAMIL)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨WSIå¤šå®ä¾‹å­¦ä¹ ä¸­å¼•å…¥â€˜åŸå‹æ³¨æ„åŠ›â€™ï¼Œå®ç°ç—…ç†åˆ‡ç‰‡çš„å¼±ç›‘ç£å¯è§£é‡Šåˆ†ç±»ã€‚
  - Intro (EN): Prototype attention for MIL on whole-slide images, enabling weakly supervised, interpretable pathology classification.
 
<img width="1312" height="583" alt="image" src="https://github.com/user-attachments/assets/9f56e1ac-1b2e-4693-8f12-e6edf834b4b4" />

- [[2024-TIP]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10684084&casa_token=o3YFdb-f9QMAAAAA:S3kJ7gaQ-9UCvVoVhArWgi_sNYy3UvdflQL8_wrNWh4Dx5CI9V_379RWNQKe5Nc_gNH_BwwoYg) **Learning transferable conceptual prototypes for interpretable unsupervised domain adaptation** [:octocat:](https://drive.google.com/file/d/1b1EHFghiF1ExD-Cn1HYg75VutfkXWp60/view?usp=sharing)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å­¦ä¹ å¯è¿ç§»/å¯è·¨åŸŸçš„æ¦‚å¿µåŸå‹ï¼Œç”¨äºæ— ç›‘ç£åŸŸè‡ªé€‚åº”ä¸è§£é‡Šã€‚
  - Intro (EN): Learns transferable conceptual prototypes for unsupervised domain adaptation with transparent reasoning.
 
<img width="1505" height="497" alt="image" src="https://github.com/user-attachments/assets/d1ad57d4-257a-4ac1-818f-01bbd9bba655" />

- [[2025-TMI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887396&casa_token=k4AqViOwfRMAAAAA:HMgKNev3t3lNTofW5OCWKhlzvyqv5Gg0JQaIV5iqu77j_iMDYr8Ng6cg2uIxYVS3xljVRjb8qA&tag=1) **Cross-and intra-image prototypical learning for multi-label disease diagnosis and interpretation** [:octocat:](https://github.com/cwangrun/CIPL)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŒæ—¶åœ¨å›¾å†…/å›¾é—´å­¦ä¹ åŸå‹ï¼Œæ”¯æŒå¤šæ ‡ç­¾ç–¾ç—…è¯Šæ–­ä¸è§£é‡Šã€‚
  - Intro (EN): Learns prototypes within and across images to support multi-label diagnosis with explanations.
<img width="1328" height="618" alt="image" src="https://github.com/user-attachments/assets/aebb6e22-4d19-469b-98fd-db5fba698a41" />

## <div id = "s10"></div> J. Medical imaging 

### <div id = "s101"></div> J1. Brain â€” Alzheimerâ€™s (MRI) 

- [[2024-MICCAI]](https://arxiv.org/pdf/2403.18328) **Pipnet3d: Interpretable detection of alzheimer in mri scans** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥Patchä¸ºç²’åº¦å­¦ä¹ ç›´è§‚åŸå‹ï¼Œçªå‡ºâ€œå±€éƒ¨è´´ç‰‡â€”æ¦‚å¿µâ€çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿äººå·¥æ ¡éªŒä¸çº é”™ã€‚
  - Intro (EN): Learns intuitive patch-level prototypes mapping local image patches to human-meaningful parts for easy auditing and correction.
    
<img width="1026" height="458" alt="image" src="https://github.com/user-attachments/assets/c17e9ecc-2ddd-464a-aa7f-002f2845bf7f" />

### <div id = "s102"></div> J2. Brain â€” Tumor (MRI / mpMRI) 

- [[2024-MIDL]](https://proceedings.mlr.press/v227/wei24a/wei24a.pdf) **Mprotonet: A case-based interpretable model for brain tumor classification with 3d multi-parametric magnetic resonance imaging** [:octocat:](https://github.com/aywi/mprotonet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: 3Då¤šå‚æ•°MRIä¸Šçš„ç—…ä¾‹å¼åŸå‹æ¨¡å‹ï¼Œä¸ºè„‘è‚¿ç˜¤åˆ†ç±»æä¾›å¯è§£é‡Šè¯æ®ã€‚
  - Intro (EN): Case-based prototype model on 3D multi-parametric MRI for brain tumor classification.
  
<img width="1760" height="902" alt="image" src="https://github.com/user-attachments/assets/264db642-df25-418a-a918-b26e40d2ea31" />

### <div id = "s103"></div> J3. Breast â€” Digital Mammography 

- [[2021-NMI]](https://arxiv.org/pdf/2103.12308) **A case-based interpretable deep learning model for classification of mass lesions in digital mammography** [:octocat:](https://github.com/alinajadebarnett/iaiabl)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨æ•°å­—ä¹³è…ºXçº¿ä¸­æ„å»ºç—…ä¾‹å¼åŸå‹è¯æ®ï¼Œè¾…åŠ©è‰¯æ¶æ€§è‚¿å—åˆ†ç±»å¹¶æä¾›å¯è§†åŒ–è§£é‡Šã€‚
  - Intro (EN): Case-based prototype evidence for digital mammography, assisting benign/malignant mass classification with visual rationale.
 
<img width="1258" height="447" alt="image" src="https://github.com/user-attachments/assets/bab2cb08-960d-45a9-8148-d6f9356a8bb1" />

- [[2024-CVPR Workshop]](https://openaccess.thecvf.com/content/CVPR2024W/DEF-AI-MIA/papers/Yang_FPN-IAIA-BL_A_Multi-Scale_Interpretable_Deep_Learning_Model_for_Classification_of_CVPRW_2024_paper.pdf) **FPN-IAIA-BL: A Multi-Scale Interpretable Deep Learning Model for Classification of Mass Margins in Digital Mammography** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨ä¹³è…ºé’¼é¶çš„â€˜è‚¿å—è¾¹ç¼˜â€™åˆ†ç±»ä¸­å¼•å…¥å¤šå°ºåº¦åŸå‹ï¼Œå…¼é¡¾å±€éƒ¨çº¹ç†ä¸å…¨å±€ç»“æ„ã€‚
  - Intro (EN): Multi-scale prototype model for breast mass margin classification, capturing local texture and global structure.
    
<img width="1030" height="365" alt="image" src="https://github.com/user-attachments/assets/32da1ba6-136d-4861-9b41-0b0bbcf2ecad" />

### <div id = "s104"></div> J4. Chest â€” Chest X-ray (CXR/COVID-19) 

- [[2021-CVPR]](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_XProtoNet_Diagnosis_in_Chest_Radiography_With_Global_and_Local_Explanations_CVPR_2021_paper.pdf) **Xprotonet: diagnosis in chest radiography with global and local explanations** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘èƒ¸éƒ¨Xå…‰çš„åŸå‹ç½‘ç»œï¼šåŒæ—¶ç»™å‡ºå…¨å±€ä¸å±€éƒ¨çš„å¯è§†åŒ–è¯æ®ï¼Œé€‚é…ä¸´åºŠåˆ¤è¯»ä¹ æƒ¯ã€‚
  - Intro (EN): Prototype network for chest radiographs: delivers both global and local visual rationales aligned with clinical reading.
 
<img width="1273" height="497" alt="image" src="https://github.com/user-attachments/assets/801c78b4-35a5-4a84-9251-94e0f33229d1" />

- [[2021-IEEE Access]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448270) **An Interpretable Deep Learning Model for Covid-19 Detection With Chest X-Ray Images** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å°†åŸå‹å¼è¯æ®ç”¨äºCOVID-19èƒ¸ç‰‡åˆ†ç±»ï¼Œå±•ç¤ºç—…ä¾‹å¼å¯¹ç…§ä¸å¯è§†åŒ–è¯´æ˜ã€‚
  - Intro (EN): Applies case-based prototype evidence to COVID-19 CXR classification with visual justifications.
    
<img width="1638" height="572" alt="image" src="https://github.com/user-attachments/assets/a443cf5f-20bf-4e1b-975a-396ba9cfda88" />

### <div id = "s105"></div> J5. Cephalometric / Dental X-ray (Keypoint) 

- [[2024-MICCAI]](https://arxiv.org/pdf/2406.12577) **Cephalometric Landmark Detection across Ages with Prototypical Network** [:octocat:](https://github.com/ShanghaiTech-IMPACT/CeLDA)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘å¤´å½±æµ‹é‡çš„å…³é”®ç‚¹æ£€æµ‹ï¼Œå¼•å…¥åŸå‹å…ˆéªŒä»¥æå‡è·¨å¹´é¾„æ³›åŒ–ã€‚
  - Intro (EN): Keypoint localization for cephalometric analysis with prototype priors to generalize across ages.
 
<img width="1350" height="790" alt="image" src="https://github.com/user-attachments/assets/27ff2f1f-733b-4fea-8705-7743fcac9306" />

### <div id = "s106"></div> J6. Pathology â€” Whole Slide Images (WSI / MIL) 

- [[2023-JBHI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10290723&casa_token=3ayrYdj6bekAAAAA:ND9zMo8PowTyi_hywubBzd7YzKCFIp4hlVR3PI6yDoVBye3j6SCoequKrnACUhRoPQp7H_cP3A) **Interpretable inference and classification of tissue types in histological colorectal cancer slides based on ensembles adaptive boosting prototype tree** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŸºäºé›†æˆ/Boostingçš„åŸå‹æ ‘ï¼Œåœ¨ç—…ç†åˆ‡ç‰‡ä¸Šè§£é‡Šç»„ç»‡ç±»å‹åˆ¤åˆ«ã€‚
  - Intro (EN): Ensemble/boosted prototype trees for tissue classification on histology slides with transparent rules.
    
<img width="1414" height="992" alt="image" src="https://github.com/user-attachments/assets/85a1c314-6646-44b4-a0ac-6c14164a3770" />

### <div id = "s107"></div> J7. Retina â€” Fundus Imaging 

- [[2025-JBHI_UR]](https://arxiv.org/pdf/2506.10669) **PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis** [:octocat:](https://github.com/marziehoghbaie/PiPViT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨çœ¼åº•å›¾åƒä¸ŠæŠŠPatchåŸå‹ä¸ViTç»“åˆï¼Œå…¼é¡¾ç»†ç²’åº¦ç—…å˜ä¸å…¨å±€ç»“æ„ã€‚
  - Intro (EN): Combines patch prototypes and ViTs for retinal imaging to capture fine lesions and global structures.
    
<img width="2029" height="672" alt="image" src="https://github.com/user-attachments/assets/1f8556e8-2a38-447c-b576-58b0be002d95" />

### <div id = "s108"></div> J8. Dermatology â€” Skin Lesions 

- [[2023-MICCAIW]](https://arxiv.org/pdf/2402.01410) **XAI for Skin Cancer Detection with Prototypes and Non-Expert Supervision** [:octocat:](https://github.com/MiguelC23/XAI-Skin-Cancer-Detection-A-Prototype-Based-Deep-Learning-Architecture-with-Non-Expert-Supervision)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨çš®è‚¤ç—…å˜åˆ†æä¸­å¼•å…¥åŸå‹ä¸éä¸“å®¶æ ‡æ³¨ï¼Œæ¢ç´¢ä½æˆæœ¬å¯è§£é‡Šè¯Šæ–­ã€‚
  - Intro (EN): Applies prototypes to skin lesion analysis with non-expert supervision to lower annotation cost.
  - 
<img width="1145" height="592" alt="image" src="https://github.com/user-attachments/assets/0fa79af0-ddff-4355-bb99-46ed491c9b42" />

### <div id = "s109"></div> J9. General / Multi-organ Frameworks (Medical) 

- [[2021-CCAI]](https://assets.pubpub.org/gzl17h3e/21624570427985.pdf) **Using ProtoPNet for Interpretable Alzheimer's Disease Classification**
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘åŒ»å­¦å½±åƒåœºæ™¯ï¼›é€šè¿‡å°†å›¾åƒå±€éƒ¨ä¸å­¦ä¹ åˆ°çš„åŸå‹åŒ¹é…ï¼Œç»™å‡ºâ€˜è¿™çœ‹èµ·æ¥åƒé‚£â€™çš„ç›´è§‚è¯æ®ã€‚
  - Intro (EN): Medical imaging focus. Matches image regions to learned prototypes to provide intuitive 'this looks like that' evidence.
 
<img width="1056" height="583" alt="image" src="https://github.com/user-attachments/assets/99e9312c-8ad2-4fcd-982b-3a3e67829a8e" />

- [[2023-IPMI]](https://arxiv.org/pdf/2303.07125) **Don't panic: Prototypical additive neural network for interpretable classification of alzheimer's disease** [:octocat:](https://github.com/ai-med/PANIC)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨ADè¯Šæ–­ä¸­æå‡ºåŠ æ€§åŸå‹ç»“æ„ï¼Œæä¾›ç—…ä¾‹å¼è¯æ®å¹¶å¢å¼ºé²æ£’æ€§ã€‚
  - Intro (EN): Introduces an additive prototype architecture for AD diagnosis with case-based evidence and robustness.
 
<img width="1018" height="416" alt="image" src="https://github.com/user-attachments/assets/b43f02c3-3a3a-4470-a957-86ed92779689" />

- [[2023-TMI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10225391&casa_token=4OiWhQrFMNAAAAAA:H1Mlkp0_Epb1XXxRVOgfKA2aUS9-Znn6z-Ki2qRb5dR56te1mXNQgzjOWNK2_kbUvtkA9KgC2w) **An Interpretable and Accurate Deep-Learning Diagnosis Framework Modeled With Fully and Semi-Supervised Reciprocal Learning** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºå…¨/åŠç›‘ç£äº’æƒ å­¦ä¹ æ¡†æ¶ï¼Œä¸åŸå‹è¯æ®ç»“åˆä»¥å…¼é¡¾æ€§èƒ½ä¸å¯è§£é‡Šã€‚
  - Intro (EN): Fully & semi-supervised reciprocal learning combined with prototype evidence for a balanced accuracy-interpretability tradeoff.
 
<img width="893" height="272" alt="image" src="https://github.com/user-attachments/assets/b69c4f8c-3020-4872-b831-e73c07602af9" />

- [[2024-arXiv]](https://arxiv.org/pdf/2404.08917?) **Maprotonet: A multi-scale attentive interpretable prototypical part network for 3d magnetic resonance imaging brain tumor classification** [:octocat:](https://github.com/TUAT-Novice/maprotonet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘3Dè„‘è‚¿ç˜¤MRIçš„å¤šå°ºåº¦æ³¨æ„åŠ›åŸå‹ç½‘ç»œï¼Œç»™å‡ºä½“ç´ /å—çº§çš„ç—…ä¾‹å¼è¯æ®ã€‚
  - Intro (EN): Multi-scale attentive prototypical parts for 3D brain tumor MRI with case-based volumetric evidence.
 
<img width="1196" height="595" alt="image" src="https://github.com/user-attachments/assets/f7dacfa1-c1b8-4f50-a657-91c1b72f22ab" />

- [[2024-CEURW]](https://arxiv.org/pdf/2407.14277) **Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for Alzheimer's Disease classification** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘åŒ»å­¦å½±åƒåœºæ™¯ã€å¼ºè°ƒå±€éƒ¨Patch/åƒç´ æˆ–å¯é€†/å¯å½¢å˜æœºåˆ¶ï¼›é€šè¿‡å°†å›¾åƒå±€éƒ¨ä¸å­¦ä¹ åˆ°çš„åŸå‹åŒ¹é…ï¼Œç»™å‡ºâ€˜è¿™çœ‹èµ·æ¥åƒé‚£â€™çš„ç›´è§‚è¯æ®ã€‚
  - Intro (EN): Medical imaging focus; Emphasizes patches/pixels, or deformable/invertible mechanisms. Matches image regions to learned prototypes to provide intuitive 'this looks like that' evidence.
 
<img width="1327" height="877" alt="image" src="https://github.com/user-attachments/assets/07cb35e1-b54f-476f-8871-025b291cfb01" />

- [[2025-JBHI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10856378&casa_token=1z-wtqmro8kAAAAA:kIrMO-_DNuwAtVTtMmuFJzqVb1kvvx3JzLojqSi16xSCmoTfNpQjK43THD93A_VQjXYGAxt3ew) **FeaInfNet: Diagnosis of Medical Images with Feature-Driven Inference and Visual Explanations** [:octocat:](https://github.com/YTPLAB/FeaInfNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥ç‰¹å¾é©±åŠ¨æ¨ç†ç»“åˆå¯è§†åŒ–è§£é‡Šï¼Œç”¨äºå¤šç±»åŒ»å­¦è¯Šæ–­ã€‚
  - Intro (EN): Feature-driven inference with visual explanations for multi-class medical diagnosis.
 
<img width="1042" height="655" alt="image" src="https://github.com/user-attachments/assets/6035a02f-c9e7-43fa-930c-cd2c4a1a5cb4" />

- [[2025-TMI]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887396&casa_token=k4AqViOwfRMAAAAA:HMgKNev3t3lNTofW5OCWKhlzvyqv5Gg0JQaIV5iqu77j_iMDYr8Ng6cg2uIxYVS3xljVRjb8qA&tag=1) **Cross-and intra-image prototypical learning for multi-label disease diagnosis and interpretation** [:octocat:](https://github.com/cwangrun/CIPL)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŒæ—¶åœ¨å›¾å†…/å›¾é—´å­¦ä¹ åŸå‹ï¼Œæ”¯æŒå¤šæ ‡ç­¾ç–¾ç—…è¯Šæ–­ä¸è§£é‡Šã€‚
  - Intro (EN): Learns prototypes within and across images to support multi-label diagnosis with explanations.
    
<img width="1308" height="626" alt="image" src="https://github.com/user-attachments/assets/afd83cea-9819-48b6-a1d1-d6d4eea2f8c9" />

## <div id = "s11"></div> K. Patch/Part/Pixel 

- [[2021-SIGKDD]](https://dl.acm.org/doi/pdf/10.1145/3447548.3467245) **Protopshare: Prototypical parts sharing for similarity discovery in interpretable image classification** [:octocat:](https://github.com/gmum/ProtoPShare)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºâ€œåŸå‹éƒ¨ä»¶å…±äº«â€ï¼Œåœ¨ç±»åˆ«ä¹‹é—´å…±äº«æˆ–è¿ç§»å…¸å‹éƒ¨ä»¶ï¼Œæå‡æ ·æœ¬æ•ˆç‡ä¸å¯è§£é‡Šæ€§ã€‚
  - Intro (EN): Proposes prototypical part sharing across classes to improve sample efficiency and interpretability.

<img width="1202" height="457" alt="image" src="https://github.com/user-attachments/assets/af7694d4-5b2b-4cf1-b004-fde7600f93ef" />

- [[2023-CVPR]](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) **Pip-net: Patch-based intuitive prototypes for interpretable image classification** [:octocat:](https://github.com/M-Nauta/PIPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ä»¥Patchä¸ºç²’åº¦å­¦ä¹ ç›´è§‚åŸå‹ï¼Œçªå‡ºâ€œå±€éƒ¨è´´ç‰‡â€”æ¦‚å¿µâ€çš„å¯¹åº”å…³ç³»ï¼Œæ–¹ä¾¿äººå·¥æ ¡éªŒä¸çº é”™ã€‚
  - Intro (EN): Learns intuitive patch-level prototypes mapping local image patches to human-meaningful parts for easy auditing and correction.

<img width="1665" height="441" alt="image" src="https://github.com/user-attachments/assets/7f70f2fc-76c1-4ca1-9ab8-117b2acc0353" />

- [[2023-ICCV]](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Evaluation_and_Improvement_of_Interpretability_for_Self-Explainable_Part-Prototype_Networks_ICCV_2023_paper.pdf) **Evaluation and improvement of interpretability for self-explainable part-prototype networks** [:octocat:](https://github.com/hqhQAQ/EvalProtoPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºåŸå‹æ¨¡å‹çš„é‡åŒ–è¯„ä¼°ä¸æ”¹è¿›æ‰‹æ®µï¼Œå…³æ³¨åŸå‹è´¨é‡ã€ç¨³å®šæ€§å’Œå¯è¯»æ€§ã€‚
  - Intro (EN): Quantitatively evaluates and improves interpretability of self-explainable part-prototype networks.

<img width="1705" height="696" alt="image" src="https://github.com/user-attachments/assets/ea5c6cae-5136-4f4b-92ed-560d99533591" />

- [[2023-ICLR]](https://openreview.net/forum?id=oiwXWPDTyNk) **Concept-level Debugging of Part-Prototype Networks** [:octocat:](https://github.com/abonte/protopdebug)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨â€˜æ¦‚å¿µå±‚â€™è°ƒè¯•åŸå‹ç½‘ç»œï¼šå‘ç°å¹¶ä¿®å¤è¯¯å¯¼æ€§æ¦‚å¿µï¼Œä»è€Œæ”¹å–„é¢„æµ‹ä¸è§£é‡Šã€‚
  - Intro (EN): Debugs part-prototype networks at the concept level, fixing misleading concepts to improve both accuracy and explanations.

<img width="1435" height="479" alt="image" src="https://github.com/user-attachments/assets/08d1e8ae-3981-424a-9866-701dbe107d24" />

- [[2023-WACV]](https://openaccess.thecvf.com/content/WACV2023/papers/Sacha_ProtoSeg_Interpretable_Semantic_Segmentation_With_Prototypical_Parts_WACV_2023_paper.pdf) **Protoseg: Interpretable semantic segmentation with prototypical parts** [:octocat:](https://github.com/gmum/proto-segmentation)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠâ€˜éƒ¨ä»¶åŸå‹â€™æ‰©å±•åˆ°è¯­ä¹‰åˆ†å‰²ï¼Œåƒç´ çº§æ˜¾ç¤ºâ€œè¿™å—åŒºåŸŸåƒå“ªç§åŸå‹â€ã€‚
  - Intro (EN): Extends part-prototypes to semantic segmentation, grounding 'which region looks like which prototype' at pixel level.

<img width="1595" height="608" alt="image" src="https://github.com/user-attachments/assets/074b11c2-6ef4-40e5-8cb1-fff90f0225ad" />

- [[2023-xAI]](https://arxiv.org/pdf/2307.14517) **The co-12 recipe for evaluating interpretable part-prototype image classifiers** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: ç»™å‡ºè¯„æµ‹åŸå‹æ–¹æ³•çš„ä¸€å¥—â€˜é…æ–¹â€™ä¸åè®®ï¼Œå¼ºè°ƒå¯å¤ç°ã€å¯é‡åŒ–çš„å¯¹æ¯”ã€‚
  - Intro (EN): Provides a reproducible 'recipe' to evaluate part-prototype methods with fair, quantitative comparisons.

<img width="1114" height="755" alt="image" src="https://github.com/user-attachments/assets/5dcfb888-7026-451c-9949-245632f75c50" />

- [[2024-AAAI]](https://ojs.aaai.org/index.php/AAAI/article/view/30154) **Interpretability benchmark for evaluating spatial misalignment of prototypical parts explanations** [:octocat:](https://github.com/gmum/interpretability-benchmark)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºç©ºé—´é”™ä½è¯„æµ‹åŸºå‡†ï¼šåŸå‹æ¿€æ´»ä¸çœŸå®éƒ¨ä½ä¸å¯¹é½çš„é—®é¢˜è¢«é‡åŒ–å¹¶å¯æ¯”è¾ƒã€‚
  - Intro (EN): Benchmarks spatial misalignment where prototype activations fail to align with true evidence regions.

<img width="1506" height="755" alt="image" src="https://github.com/user-attachments/assets/ca707e67-fd7d-4267-8086-beb0f3336a3c" />

- [[2024-arXiv]](https://arxiv.org/pdf/2404.08917?) **Maprotonet: A multi-scale attentive interpretable prototypical part network for 3d magnetic resonance imaging brain tumor classification** [:octocat:](https://github.com/TUAT-Novice/maprotonet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘3Dè„‘è‚¿ç˜¤MRIçš„å¤šå°ºåº¦æ³¨æ„åŠ›åŸå‹ç½‘ç»œï¼Œç»™å‡ºä½“ç´ /å—çº§çš„ç—…ä¾‹å¼è¯æ®ã€‚
  - Intro (EN): Multi-scale attentive prototypical parts for 3D brain tumor MRI with case-based volumetric evidence.

<img width="1196" height="595" alt="image" src="https://github.com/user-attachments/assets/f7dacfa1-c1b8-4f50-a657-91c1b72f22ab" />

- [[2024-biomedinformatics]](https://www.mdpi.com/2673-7426/4/4/115) **Part-Prototype Models in Medical Imaging Applications and Current Challenges** [:octocat:](#)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŒ»å­¦å½±åƒåŸå‹æ–¹æ³•ç»¼è¿°ï¼Œç³»ç»Ÿæ¢³ç†åº”ç”¨ã€æŒ‘æˆ˜ä¸æœªæ¥æ–¹å‘ã€‚
  - Intro (EN): Survey of prototype-based methods in medical imaging: applications, challenges, and future directions.

<img width="1465" height="587" alt="image" src="https://github.com/user-attachments/assets/a658d102-231a-413e-8524-70077f43928c" />

- [[2024-CEURW]](https://arxiv.org/pdf/2407.14277) **Patch-based Intuitive Multimodal Prototypes Network (PIMPNet) for Alzheimer's Disease classification** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é¢å‘åŒ»å­¦å½±åƒåœºæ™¯ã€å¼ºè°ƒå±€éƒ¨Patch/åƒç´ æˆ–å¯é€†/å¯å½¢å˜æœºåˆ¶ï¼›é€šè¿‡å°†å›¾åƒå±€éƒ¨ä¸å­¦ä¹ åˆ°çš„åŸå‹åŒ¹é…ï¼Œç»™å‡ºâ€˜è¿™çœ‹èµ·æ¥åƒé‚£â€™çš„ç›´è§‚è¯æ®ã€‚
  - Intro (EN): Medical imaging focus; Emphasizes patches/pixels, or deformable/invertible mechanisms. Matches image regions to learned prototypes to provide intuitive 'this looks like that' evidence.

<img width="1342" height="871" alt="image" src="https://github.com/user-attachments/assets/deb95825-bb48-4afe-92fb-330a670d5bbd" />

- [[2024-ICML]](https://openreview.net/pdf?id=jhWSzTO0Jl) **Post-hoc Part-Prototype Networks** [[other source]](https://hkustsmartlab.github.io/2024/06/08/icml/)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨é»‘ç›’æ¨¡å‹ä¹‹ä¸ŠåéªŒæ„å»ºâ€˜éƒ¨ä»¶-åŸå‹â€™è§£é‡Šå±‚ï¼Œæ— éœ€é‡è®­ä¸»å¹²ã€‚
  - Intro (EN): Builds a post-hoc part-prototype layer on top of a black-box model without retraining the backbone.

<img width="1362" height="1028" alt="image" src="https://github.com/user-attachments/assets/e06572f9-c909-4817-a6f6-2b9f9bf647c0" />

- [[2024-IJCAI]](https://www.ijcai.org/proceedings/2024/168) **ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognitio** [:octocat:](https://github.com/zju-vipa/ProtoPFormer)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨ViTä¸Šä¸“æ³¨äºåŸå‹åŒ–çš„å±€éƒ¨éƒ¨ä»¶ï¼ŒæŠ‘åˆ¶èƒŒæ™¯å¹²æ‰°ï¼Œæå‡è§£é‡Šèšç„¦åº¦ã€‚
  - Intro (EN): Prototype-focused ViT that suppresses background distraction, sharpening attention on meaningful parts.

<img width="1200" height="480" alt="image" src="https://github.com/user-attachments/assets/8246c5d1-feb1-460b-8ec7-69961b19d9f3" />

- [[2024-WACV]](https://openaccess.thecvf.com/content/WACV2024/papers/Carmichael_Pixel-Grounded_Prototypical_Part_Networks_WACV_2024_paper.pdf) **Pixel-grounded prototypical part networks** [:octocat:](https://github.com/merlresearch/PixPNet)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å°†åŸå‹æ˜¾å¼å¯¹é½åˆ°åƒç´ å±‚çº§ä¸å®ä¾‹è½®å»“ï¼Œç¼“è§£â€˜æ¿€æ´»å¤–æº¢â€™é—®é¢˜ã€‚
  - Intro (EN): Aligns prototypes to pixels and instance contours, mitigating activation spillover.

<img width="1840" height="525" alt="image" src="https://github.com/user-attachments/assets/3140a92b-334a-4ecd-a2d5-a4bf16c2424c" />

- [[2024-xAI]](https://arxiv.org/pdf/2408.11401) **Revisiting FunnyBirds evaluation framework for prototypical parts networks** [:octocat:](https://github.com/hamer101/FunnyBirds_PrototypesRevisited)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: é‡å®¡FunnyBirdsæ¡†æ¶ï¼Œæç¤ºåŸå‹æ–¹æ³•çš„è¯„æµ‹é™·é˜±ä¸æ”¹è¿›æ–¹å‘ã€‚
  - Intro (EN): Revisits the FunnyBirds framework to highlight pitfalls and fixes in prototype evaluation.

<img width="924" height="299" alt="image" src="https://github.com/user-attachments/assets/c2a48b79-174b-42d2-82df-7927ac82ef7e" />

- [[2025-JBHI_UR]](https://arxiv.org/pdf/2506.10669) **PiPViT: Patch-based Visual Interpretable Prototypes for Retinal Image Analysis** [:octocat:](https://github.com/marziehoghbaie/PiPViT)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨çœ¼åº•å›¾åƒä¸ŠæŠŠPatchåŸå‹ä¸ViTç»“åˆï¼Œå…¼é¡¾ç»†ç²’åº¦ç—…å˜ä¸å…¨å±€ç»“æ„ã€‚
  - Intro (EN): Combines patch prototypes and ViTs for retinal imaging to capture fine lesions and global structures.

<img width="2029" height="672" alt="image" src="https://github.com/user-attachments/assets/1f8556e8-2a38-447c-b576-58b0be002d95" />

- [[2025-CVPR]](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhu_Interpretable_Image_Classification_via_Non-parametric_Part_Prototype_Learning_CVPR_2025_paper.pdf) **Interpretable Image Classification via Non-parametric Part Prototype Learning** [:octocat:](https://github.com/zijizhu/proto-non-param)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: å­¦ä¹ éå‚æ•°çš„éƒ¨ä»¶åŸå‹é›†åˆï¼Œå‡å°‘å¼ºå‡è®¾å¹¶æå‡å¯ç¼–è¾‘æ€§ã€‚
  - Intro (EN): Learns non-parametric sets of part prototypes, reducing assumptions and enhancing editability.

<img width="1389" height="776" alt="image" src="https://github.com/user-attachments/assets/5fdc93f5-d1dd-4b19-931c-db51a6ee8f39" />

- [[2025-CVPR]](https://openaccess.thecvf.com/content/CVPR2025/papers/Donnelly_Rashomon_Sets_for_Prototypical-Part_Networks_Editing_Interpretable_Models_in_Real-Time_CVPR_2025_paper.pdf) **Rashomon sets for prototypical-part networks: Editing interpretable models in real-time** [:octocat:](https://github.com/jdonnelly36/proto-rset)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åˆ»ç”»åŸå‹ç½‘ç»œçš„â€˜æ‹‰ç»è’™é›†åˆâ€™ï¼Œæ”¯æŒåœ¨ç­‰ç²¾åº¦è§£ç©ºé—´å†…è¿›è¡Œå¯æ§/å®æ—¶ç¼–è¾‘ã€‚
  - Intro (EN): Characterizes Rashomon sets for prototype networks, enabling controlled, real-time edits within equal-accuracy solutions.

<img width="779" height="439" alt="image" src="https://github.com/user-attachments/assets/03aca78c-3e4c-49f6-a2e4-e4b6c61e200b" />

- [[2025-CVPRW]](https://openaccess.thecvf.com/content/CVPR2025W/TCV/papers/Singh_ProtoPatchNet_An_Interpretable_Patch-Based_Prototypical_Network_CVPRW_2025_paper.pdf) **ProtoPatchNet: An Interpretable Patch-Based Prototypical Network** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åŸºäºPatchçš„åŸå‹ç½‘ç»œï¼Œå¼ºè°ƒå¯ç»„åˆçš„å±€éƒ¨è¯æ®ä¸ç®€æ´çš„å¯è§†åŒ–ã€‚
  - Intro (EN): Patch-based prototypical network with composable local evidence and concise visualization.

<img width="1388" height="532" alt="image" src="https://github.com/user-attachments/assets/5c7f7211-3d6d-444b-b11e-f6af5556e916" />

- [[2025-ICASSP]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890753&casa_token=CK3WkOK9TVsAAAAA:pBren1C90XkdKu7gUBnv5ReeSpxZ0zmhf2FeYqh-2t0gPXfpQPYsMb9onOdSyABmObSQkB_7dQ) **Prototypical Part Transformer for Interpretable Image Recognition**
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æŠŠéƒ¨ä»¶åŸå‹ä¸Transformerç»“æ„ç»“åˆï¼Œç»Ÿä¸€è¡¨ç¤ºå­¦ä¹ ä¸è§£é‡Šã€‚
  - Intro (EN): Integrates part prototypes into a Transformer, unifying representation and explanation.

<img width="1275" height="417" alt="image" src="https://github.com/user-attachments/assets/0178283a-d91c-49a4-8657-85b17888c718" />

- [[2025-ICCVW_BEW]](https://openreview.net/forum?id=PcF9Q51rF1&noteId=KSZAX91CoY) **HAPPI: Hyperbolic Hierarchical Part Prototypes for Image Recognition** 
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: åœ¨è¶…æ›²å‡ ä½•ä¸­å­¦ä¹ å±‚çº§åŸå‹ï¼Œæå‡å±‚æ¬¡ç»“æ„è¡¨è¾¾ä¸ç›¸ä¼¼åº¦åº¦é‡ã€‚
  - Intro (EN): Learns hierarchical prototypes in hyperbolic geometry to better encode tree-like relations.

<img width="1603" height="716" alt="image" src="https://github.com/user-attachments/assets/0c690ab4-4ffc-42a4-9d0d-024ea040a7f9" />

- [[2025-ICLR]](https://openreview.net/forum?id=BM9qfolt6p) **LucidPPN: Unambiguous prototypical parts network for user-centric interpretable computer vision** [:octocat:](https://github.com/mateuszpach/LucidPPN)
  - ç®€ä»‹ï¼ˆä¸­æ–‡ï¼‰: æå‡ºâ€˜æ— æ­§ä¹‰â€™åŸå‹éƒ¨ä»¶å­¦ä¹ ï¼Œå‡å°‘åŸå‹æ··æ·†å¹¶æé«˜äººç±»å¯è¯»æ€§ã€‚
  - Intro (EN): Targets unambiguous prototype parts to reduce confusion and improve human readability.

<img width="1137" height="847" alt="image" src="https://github.com/user-attachments/assets/76941c75-239b-46ad-a88b-27bb84481950" />

## ğŸ™ Acknowledgements
We thank the authors of the cited papers and the open-source community for their contributions.  
We also appreciate everyone who has submitted PRs and issues.  
This list is compiled for academic reference only; copyrights for the papers belong to their respective authors and publishers.

## ğŸ¤ Contributing
PRs/Issues are welcome! Please follow this minimal schema:
```
- year: 2025
  venue: CVPR
  title: Interpretable Image Classification via Non-parametric Part Prototype Learning
  link_pdf: https://...
  link_code: https://github.com/...
  tags: [K, D2]        # Aâ€“K (multi-label), and J subcategories if applicable
  tasks: [classification, segmentation]
  modality: [natural, fundus, WSI]
  notes_en: One-line summary
```

## ğŸ’ Citation

```
@misc{jia2025,
  author = {Jia, Junhao and Sun, Yifei and Liu, Yunyou and Chen, Huangwei and Jiang, Shuo and Lin, Huangxing and Luo, Ziyan and Zheng, Hanwen and Shi, Yuting},
  title = {Paper List for Prototypical Learning},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/BeistMedAI/Paper-List-for-Prototypical-Learning}}
}
```

  ## ğŸ¥° Star History
<div id = "s0"></div>

[![Star History Chart](https://api.star-history.com/svg?repos=BeistMedAI/Paper-List-for-Prototypical-Learning&type=Timeline)](https://www.star-history.com/#BeistMedAI/Paper-List-for-Prototypical-Learning&Timeline)

[![](https://capsule-render.vercel.app/api?type=waving&height=200&color=0:0F172A,65:4F46E5,100:22D3EE&text=Back%20to%20Top&section=footer&fontSize=30&fontAlignY=65&fontColor=FFFFFF)](#top)
